- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: What does RDD stand for in Spark?
  options:
    '1': Resilient Distributed Dataset
    '2': Reliable Data Division
    '3': Random Data Distribution
    '4': Resilient Data Division
  answer: '1'
  id: 1

- stack: spark
  type: mcq
  topic: Spark Streaming
  difficulty: hard
  question: What is Structured Streaming in Spark?
  options:
    '1': A low-level API for DStreams
    '2': A scalable stream processing engine built on Spark SQL
    '3': A separate component outside Spark Core
    '4': A Spark MLlib API for real-time ML
  answer: '2'
  id: 2

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: medium
  question: Which of the following best describes a DataFrame in Spark?
  options:
    '1': A distributed collection of key-value pairs
    '2': A distributed table with rows and named columns
    '3': A local Pandas DataFrame
    '4': A Spark ML pipeline
  answer: '2'
  id: 3

- stack: spark
  type: mcq
  topic: Spark SQL
  difficulty: easy
  question: Which Spark component allows running SQL queries?
  options:
    '1': Spark SQL
    '2': Spark Core
    '3': Spark MLlib
    '4': Spark GraphX
  answer: '1'
  id: 4

- stack: spark
  type: mcq
  topic: RDD
  difficulty: medium
  question: Which operation in Spark RDD triggers execution?
  options:
    '1': map
    '2': filter
    '3': reduce
    '4': flatMap
  answer: '3'
  id: 5

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: medium
  question: What file formats can Spark DataFrame natively read?
  options:
    '1': JSON, Parquet, ORC, CSV
    '2': Only CSV and JSON
    '3': Only Parquet
    '4': Only Avro
  answer: '1'
  id: 6

- stack: spark
  type: mcq
  topic: Basics
  difficulty: hard
  question: What is the default cluster manager for Spark?
  options:
    '1': YARN
    '2': Mesos
    '3': Standalone
    '4': Kubernetes
  answer: '3'
  id: 7

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is Catalyst in Spark?
  options:
    '1': The Spark ML optimization engine
    '2': The Spark SQL query optimizer
    '3': The Spark GraphX engine
    '4': The Spark job scheduler
  answer: '2'
  id: 8

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is Tungsten in Spark?
  options:
    '1': Sparkâ€™s memory and code optimization project
    '2': A Spark SQL function
    '3': Spark ML clustering algorithm
    '4': A deployment mode
  answer: '1'
  id: 9

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: hard
  question: Which join type is not supported in Spark SQL?
  options:
    '1': Inner
    '2': Outer
    '3': Lateral
    '4': Left Semi
  answer: '3'
  id: 10

- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: Which is an action in Spark RDD?
  options:
    '1': map
    '2': filter
    '3': count
    '4': flatMap
  answer: '3'
  id: 11

- stack: spark
  type: mcq
  topic: Spark Streaming
  difficulty: medium
  question: What is a DStream in Spark?
  options:
    '1': A stream of RDDs
    '2': A DataFrame API
    '3': A SQL query
    '4': A Spark ML pipeline
  answer: '1'
  id: 12

- stack: spark
  type: mcq
  topic: Deployment
  difficulty: medium
  question: Which of the following is NOT a Spark deployment mode?
  options:
    '1': Client
    '2': Cluster
    '3': Embedded
    '4': Local
  answer: '3'
  id: 13

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: easy
  question: What function is used to display the first rows of a DataFrame?
  options:
    '1': show()
    '2': head()
    '3': collect()
    '4': take()
  answer: '1'
  id: 14

- stack: spark
  type: mcq
  topic: Basics
  difficulty: hard
  question: What does Spark use for DAG scheduling?
  options:
    '1': Stage and Task-based scheduling
    '2': MapReduce scheduling
    '3': Thread-based scheduling
    '4': FIFO only
  answer: '1'
  id: 15

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is predicate pushdown in Spark?
  options:
    '1': Moving filters close to the data source
    '2': Pushing data to workers
    '3': Partition pruning
    '4': Repartitioning DataFrames
  answer: '1'
  id: 16

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: medium
  question: Which method persists a DataFrame in memory?
  options:
    '1': cache()
    '2': register()
    '3': save()
    '4': checkpoint()
  answer: '1'
  id: 17

- stack: spark
  type: mcq
  topic: RDD
  difficulty: hard
  question: Which transformation guarantees data shuffling in Spark?
  options:
    '1': map
    '2': filter
    '3': groupByKey
    '4': flatMap
  answer: '3'
  id: 18

- stack: spark
  type: mcq
  topic: Basics
  difficulty: medium
  question: In Spark, what is a narrow transformation?
  options:
    '1': A transformation that requires data shuffling
    '2': A transformation that doesnâ€™t require data shuffling
    '3': An action on RDD
    '4': A join operation
  answer: '2'
  id: 19

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: easy
  question: Which Spark API is most similar to SQL?
  options:
    '1': RDD API
    '2': DataFrame API
    '3': GraphX API
    '4': Streaming API
  answer: '2'
  id: 20

- stack: spark
  type: mcq
  topic: Spark SQL
  difficulty: hard
  question: In Spark SQL, which clause is used to define a window for analytic functions?
  options:
    '1': over()
    '2': window()
    '3': row_number()
    '4': rank()
  answer: '1'
  id: 21

- stack: spark
  type: mcq
  topic: Basics
  difficulty: medium
  question: Which cluster manager is NOT supported by Spark?
  options:
    '1': Kubernetes
    '2': Mesos
    '3': Hadoop YARN
    '4': Apache Flink
  answer: '4'
  id: 22

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: Which file format is best for Spark performance with schema evolution?
  options:
    '1': JSON
    '2': Parquet
    '3': CSV
    '4': TXT
  answer: '2'
  id: 23

- stack: spark
  type: mcq
  topic: Spark Streaming
  difficulty: hard
  question: How does checkpointing help in Spark Streaming?
  options:
    '1': Saves intermediate RDDs to avoid recomputation
    '2': Persists DataFrames in memory
    '3': Reduces network shuffle
    '4': Balances partitions
  answer: '1'
  id: 24

- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: Which method is used to parallelize a local collection into RDD?
  options:
    '1': SparkContext.textFile()
    '2': SparkContext.parallelize()
    '3': SparkContext.runJob()
    '4': SparkContext.range()
  answer: '2'
  id: 25

- stack: spark
  type: mcq
  topic: Dataframes
  difficulty: hard
  question: Which method writes DataFrame in append mode?
  options:
    '1': df.write.save("path")
    '2': df.write.mode("append").save("path")
    '3': df.save("path")
    '4': df.append("path")
  answer: '2'
  id: 26

- stack: spark
  type: mcq
  topic: Spark SQL
  difficulty: medium
  question: How do you register a DataFrame as a temporary SQL table?
  options:
    '1': createTempView
    '2': registerTable
    '3': registerSQL
    '4': createView
  answer: '1'
  id: 27

- stack: spark
  type: mcq
  topic: Basics
  difficulty: medium
  question: Which deployment mode runs the driver on the cluster?
  options:
    '1': Client
    '2': Cluster
    '3': Local
    '4': Embedded
  answer: '2'
  id: 28

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: hard
  question: What is broadcast join in Spark?
  options:
    '1': Joining large datasets by repartition
    '2': Sending a small dataset to all worker nodes
    '3': Streaming joins in real-time
    '4': Cross join with shuffle
  answer: '2'
  id: 29

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Which language APIs does Spark support?
  options:
    '1': Java, Scala, Python, R
    '2': Only Java and Scala
    '3': Only Python
    '4': Java, Scala, C++
  answer: '1'
  id: 30

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Big Data?
  options:
  '1': Data that fits into a single system easily
  '2': Data that is too large and complex to process with traditional systems
  '3': Only structured data from databases
  '4': Small datasets analyzed in Excel
  answer: '2'
  id: 31

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What are the 3Vs of Big Data?
  options:
  '1': Volume, Velocity, Variety
  '2': Value, Volume, Variance
  '3': Velocity, Variance, Version
  '4': Value, Variety, Version
  answer: '1'
  id: 32

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the difference between batch processing and real-time processing?
  options:
  '1': Batch processes data in bulk, real-time processes data continuously
  '2': Both process data continuously
  '3': Batch processes only structured data, real-time processes unstructured
  '4': No difference between the two
  answer: '1'
  id: 33

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Give an example of a batch processing system.
  options:
  '1': Apache Kafka
  '2': Apache Hadoop MapReduce
  '3': Apache Storm
  '4': Apache Flink
  answer: '2'
  id: 34

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Give an example of a real-time processing system.
  options:
  '1': Apache Flink
  '2': Apache Hadoop
  '3': Microsoft Excel
  '4': MySQL Database
  answer: '1'
  id: 35

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Apache Spark?
  options:
  '1': A distributed real-time monitoring system
  '2': A distributed data processing engine for big data analytics
  '3': A relational database
  '4': A NoSQL database
  answer: '2'
  id: 36

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Why is Spark faster than Hadoop MapReduce?
  options:
  '1': Spark stores data in RAM and uses DAG execution
  '2': Spark ignores data locality
  '3': Spark uses only disk-based storage
  '4': Spark does not support in-memory computation
  answer: '1'
  id: 37

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: List the main components of the Spark ecosystem.
  options:
  '1': Spark Core, Spark SQL, Spark Streaming, MLlib, GraphX
  '2': HDFS, YARN, Hive, Pig
  '3': MapReduce, HBase, Sqoop, Flume
  '4': Kafka, Zookeeper, Flink, Storm
  answer: '1'
  id: 38

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Spark Core?
  options:
  '1': The component that handles SQL queries
  '2': The foundation for Spark providing basic functionalities like task scheduling and memory management
  '3': The library for machine learning
  '4': The graph processing library
  answer: '2'
  id: 39

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Spark SQL used for?
  options:
  '1': Running SQL queries and working with structured data
  '2': Handling unstructured streaming data
  '3': Performing graph computations
  '4': Running machine learning algorithms
  answer: '1'
  id: 40

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Spark Streaming?
  options:
  '1': A library for working with static datasets
  '2': A library for processing real-time streaming data
  '3': A library for machine learning
  '4': A library for graph computation
  answer: '2'
  id: 41

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is MLlib in Spark?
  options:
  '1': A library for graph computation
  '2': A library for machine learning algorithms
  '3': A library for SQL queries
  '4': A library for distributed file storage
  answer: '2'
  id: 42

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is GraphX in Spark?
  options:
  '1': A library for streaming
  '2': A library for structured queries
  '3': A library for graph processing
  '4': A library for ML models
  answer: '3'
  id: 43

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Explain Sparkâ€™s architecture in one line.
  options:
  '1': Master-Slave architecture with Driver and Executors
  '2': Only master node executes tasks
  '3': A purely peer-to-peer system
  '4': A single-threaded execution engine
  answer: '1'
  id: 44

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the role of the Driver in Spark?
  options:
  '1': To execute tasks directly on worker nodes
  '2': To convert user code into tasks and schedule execution
  '3': To store RDDs in memory
  '4': To manage HDFS storage
  answer: '2'
  id: 45

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What are Executors in Spark?
  options:
  '1': Processes running on worker nodes executing tasks
  '2': Processes managing cluster resources
  '3': The Spark shell interface
  '4': In-memory metadata store
  answer: '1'
  id: 46

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the role of the Cluster Manager?
  options:
  '1': To manage HDFS storage
  '2': To allocate resources to applications and manage cluster nodes
  '3': To execute SQL queries
  '4': To train machine learning models
  answer: '2'
  id: 47

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the difference between Standalone and YARN in Spark?
  options:
  '1': Standalone is Sparkâ€™s own manager, YARN is Hadoopâ€™s cluster manager
  '2': Both are identical in architecture
  '3': Standalone supports only SQL, YARN supports ML
  '4': YARN runs locally while Standalone runs on clusters
  answer: '1'
  id: 48

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Spark on Kubernetes?
  options:
  '1': Running Spark applications in Docker containers orchestrated by Kubernetes
  '2': Spark integrated with Hadoop YARN
  '3': Spark replacing Kubernetes for scheduling
  '4': Spark running only on a single machine
  answer: '1'
  id: 49

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Local mode in Spark?
  options:
  '1': Running Spark on a single JVM for testing and debugging
  '2': Running Spark across multiple nodes in a cluster
  '3': Running Spark on cloud only
  '4': Running Spark without any executors
  answer: '1'
  id: 50

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Cluster mode in Spark?
  options:
  '1': Running Spark applications on multiple nodes in a distributed environment
  '2': Running Spark locally on a single node
  '3': Running Spark without a driver
  '4': Running Spark with no executors
  answer: '1'
  id: 51

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Which cluster managers can Spark run on?
  options:
  '1': Standalone, YARN, Mesos, Kubernetes
  '2': Only YARN and Standalone
  '3': Only Mesos
  '4': Only Kubernetes
  answer: '1'
  id: 52

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is Spark Shell?
  options:
  '1': An interactive REPL for running Spark commands
  '2': A command-line tool for managing Hadoop clusters
  '3': A GUI for Spark job monitoring
  '4': A debugging tool only for MLlib
  answer: '1'
  id: 53

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Which languages are supported by Spark Shell?
  options:
  '1': Scala and Python
  '2': Only Java
  '3': R and C++
  '4': SQL only
  answer: '1'
  id: 54

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Why is Spark Shell useful?
  options:
  '1': For quick testing, prototyping, and learning Spark interactively
  '2': For managing HDFS
  '3': For training ML models in production
  '4': For monitoring Spark jobs only
  answer: '1'
  id: 55

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the difference between PySpark and Scala Spark?
  options:
  '1': PySpark uses Python APIs, Scala Spark uses Scala APIs
  '2': Both are identical without any differences
  '3': PySpark is faster than Scala Spark
  '4': Scala Spark only supports streaming
  answer: '1'
  id: 56

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the role of DAG (Directed Acyclic Graph) in Spark execution?
  options:
  '1': To represent execution plan of tasks in a fault-tolerant way
  '2': To store datasets in HDFS
  '3': To run SQL queries
  '4': To monitor Spark job logs
  answer: '1'
  id: 57

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is SparkContext?
  options:
  '1': The entry point for Spark functionalities before Spark 2.0
  '2': The component for SQL queries
  '3': The API for streaming data
  '4': The ML library of Spark
  answer: '1'
  id: 58

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is SparkSession?
  options:
  '1': The unified entry point to Spark introduced in 2.0
  '2': The API only for Spark SQL
  '3': The ML pipeline API
  '4': The resource manager of Spark
  answer: '1'
  id: 59

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What is the difference between SparkContext and SparkSession?
  options:
  '1': SparkContext is the old entry point, SparkSession is the unified entry point post 2.0
  '2': Both are identical
  '3': SparkContext is only for streaming, SparkSession is for SQL
  '4': SparkSession runs only in local mode
  answer: '1'
  id: 60

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What does RDD stand for in Spark and what role does it serve?
  options:
  '1': Resilient Distributed Dataset, fundamental data structure of Spark
  '2': Random Distributed Data, temporary cache in Spark
  '3': Repeated Data Distribution, used for backups
  '4': Resilient Data Driver, Spark resource manager
  answer: '1'
  id: 61

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Why are distributed datasets called 'resilient'?
  options:
  '1': Because they can recover automatically from node failures
  '2': Because they are always stored on disk
  '3': Because they are immutable and cannot be changed
  '4': Because they support SQL queries
  answer: '1'
  id: 62

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What does it mean that distributed datasets are immutable?
  options:
  '1': Once created, they cannot be changed and only new datasets are derived
  '2': They can be updated in place at any time
  '3': They can be deleted partially on worker nodes
  '4': They allow mutable shared state across executors
  answer: '1'
  id: 63

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: What does lazy evaluation mean in Spark?
  options:
  '1': Transformations are executed immediately when called
  '2': Computation is triggered only when an action is performed
  '3': Spark delays loading libraries until runtime
  '4': Spark waits until cluster manager assigns resources before parsing code
  answer: '2'
  id: 64

- stack: spark
  type: mcq
  topic: Basics
  difficulty: medium
  question: What is the difference between narrow and wide dependencies in distributed datasets?
  options:
  '1': Narrow dependencies require shuffling, wide dependencies do not
  '2': Narrow dependencies have limited parent partitions per child, wide dependencies can depend on many parent partitions
  '3': Narrow dependencies happen in streaming, wide dependencies in batch
  '4': Narrow dependencies are used in SQL, wide dependencies in ML
  answer: '2'
  id: 65

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Give an example of a narrow dependency operation.
  options:
  '1': map()
  '2': reduceByKey()
  '3': groupByKey()
  '4': join()
  answer: '1'
  id: 66

- stack: spark
  type: mcq
  topic: Basics
  difficulty: easy
  question: Give an example of a wide dependency operation.
  options:
  '1': filter()
  '2': map()
  '3': groupByKey()
  '4': flatMap()
  answer: '3'
  id: 67

- stack: spark
  type: mcq
  topic: Basics
  difficulty: medium
  question: What is lineage in distributed datasets?
  options:
  '1': The history of transformations used to build a dataset
  '2': The physical memory location of a dataset
  '3': The version control system for datasets
  '4': The schema of structured data
  answer: '1'
  id: 68

- stack: spark
  type: mcq
  topic: Basics
  difficulty: medium
  question: How does Spark use DAG (Directed Acyclic Graph) for distributed datasets?
  options:
  '1': To represent the fault tolerance lineage of datasets
  '2': To allocate cluster resources
  '3': To store intermediate results on disk
  '4': To execute SQL queries only
  answer: '1'
  id: 69

- stack: spark
  type: mcq
  topic: Fault Tolerance
  difficulty: easy
  question: What is fault tolerance in distributed data structures?
  options:
  '1': The ability to prevent all hardware failures completely
  '2': The ability to recover lost data and computations after failures
  '3': Running computations only on a single machine to avoid faults
  '4': Replicating data endlessly without any cost
  answer: '2'
  id: 70

- stack: spark
  type: mcq
  topic: Fault Tolerance
  difficulty: medium
  question: How does Spark achieve fault tolerance in Structured Streaming?
  options:
  '1': By re-running failed queries from the beginning only
  '2': By using checkpointing and write-ahead logs to recover state and progress
  '3': By ignoring failed data records
  '4': By relying entirely on external databases
  answer: '2'
  id: 71

- stack: spark
  type: mcq
  topic: Fault Tolerance
  difficulty: medium
  question: What is checkpointing in Spark?
  options:
  '1': Saving intermediate RDDs to disk to truncate lineage for recovery
  '2': A mechanism to replicate data to all executors
  '3': A system for caching datasets in memory
  '4': A logging tool for Spark SQL queries
  answer: '1'
  id: 72

- stack: spark
  type: mcq
  topic: Fault Tolerance
  difficulty: medium
  question: What is the role of Write-Ahead Logs (WAL) in Spark Streaming?
  options:
  '1': To store log messages for debugging only
  '2': To replicate data across clusters
  '3': To ensure received data is reliably stored before processing
  '4': To execute queries faster in memory
  answer: '3'
  id: 73

- stack: spark
  type: mcq
  topic: Fault Tolerance
  difficulty: medium
  question: How does Spark recover RDDs when a node fails?
  options:
  '1': By restarting the cluster from scratch
  '2': By recomputing lost partitions using lineage information
  '3': By storing all data permanently in RAM
  '4': By automatically replicating each partition to all nodes
  answer: '2'
  id: 74

- stack: spark
  type: mcq
  topic: Transformations
  difficulty: easy
  question: What are transformations in Spark's distributed datasets?
  options:
  '1': Operations that trigger immediate computation
  '2': Operations that define a new RDD from an existing one without immediate execution
  '3': Actions that return values to the driver program
  '4': Commands to save RDDs to disk
  answer: '2'
  id: 75

- stack: spark
  type: mcq
  topic: Transformations
  difficulty: easy
  question: Give two examples of transformations in Spark.
  options:
  '1': map(), flatMap()
  '2': collect(), count()
  '3': reduce(), take()
  '4': saveAsTextFile(), count()
  answer: '1'
  id: 76

- stack: spark
  type: mcq
  topic: Transformations
  difficulty: easy
  question: What does the map() transformation do on a distributed dataset?
  options:
  '1': Returns only distinct elements
  '2': Applies a function to each element and returns a new RDD of the same size
  '3': Splits elements into multiple parts and flattens them
  '4': Collects results back to the driver
  answer: '2'
  id: 77

- stack: spark
  type: mcq
  topic: Transformations
  difficulty: easy
  question: What does the flatMap() transformation do on a distributed dataset?
  options:
  '1': Maps each element to zero or more elements and flattens the result
  '2': Filters elements based on a condition
  '3': Returns a random subset of the dataset
  '4': Joins two datasets together
  answer: '1'
  id: 78

- stack: spark
  type: mcq
  topic: Transformations
  difficulty: easy
  question: What does the filter() transformation do on a distributed dataset?
  options:
  '1': Applies a function to all elements
  '2': Returns only elements that satisfy a given condition
  '3': Groups data by key
  '4': Removes duplicates from the dataset
  answer: '2'
  id: 79

- stack: spark
  type: mcq
  topic: Actions
  difficulty: easy
  question: What are actions in Spark's distributed datasets?
  options:
  '1': Operations that define a new dataset lazily
  '2': Operations that trigger execution and return results or write output
  '3': Operations that only transform data without computing
  '4': Operations that always save data to HDFS
  answer: '2'
  id: 80

- stack: spark
  type: mcq
  topic: Actions
  difficulty: easy
  question: Give two examples of actions in Spark.
  options:
  '1': reduce(), collect()
  '2': map(), flatMap()
  '3': filter(), distinct()
  '4': join(), union()
  answer: '1'
  id: 81

- stack: spark
  type: mcq
  topic: Actions
  difficulty: medium
  question: What does reduce() action do in Spark?
  options:
  '1': Saves data to disk
  '2': Aggregates elements of an RDD using a specified function
  '3': Groups elements by key
  '4': Flattens a dataset into smaller parts
  answer: '2'
  id: 82

- stack: spark
  type: mcq
  topic: Actions
  difficulty: medium
  question: What is the difference between reduceByKey and groupByKey?
  options:
  '1': reduceByKey combines values per key locally before shuffling, groupByKey shuffles all data without pre-aggregation
  '2': groupByKey is faster and more efficient than reduceByKey
  '3': reduceByKey only works with strings, groupByKey only works with numbers
  '4': Both are identical in performance and behavior
  answer: '1'
  id: 83

- stack: spark
  type: mcq
  topic: Actions
  difficulty: easy
  question: What does the collect() action do on a distributed dataset?
  options:
  '1': Gathers all elements from workers and returns them to the driver program
  '2': Saves the dataset to HDFS
  '3': Removes duplicate elements
  '4': Splits data into partitions
  answer: '1'
  id: 84

- stack: spark
  type: mcq
  topic: Actions
  difficulty: easy
  question: What does the count() action do on a distributed dataset?
  options:
  '1': Returns the first element
  '2': Returns the total number of elements
  '3': Saves the dataset to a text file
  '4': Returns distinct elements only
  answer: '2'
  id: 85

- stack: spark
  type: mcq
  topic: Actions
  difficulty: easy
  question: What does the take() action do on a distributed dataset?
  options:
  '1': Returns a fixed number of elements from the dataset
  '2': Removes elements that donâ€™t match a condition
  '3': Saves the dataset in sequence files
  '4': Returns only distinct values
  answer: '1'
  id: 86

- stack: spark
  type: mcq
  topic: Actions
  difficulty: easy
  question: What does saveAsTextFile() action do?
  options:
  '1': Saves the RDD as a text file in the local file system or HDFS
  '2': Saves the RDD as a binary file only
  '3': Collects elements and prints them on the driver console
  '4': Returns only the schema of the dataset
  answer: '1'
  id: 87

- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: How can you create a distributed dataset from a collection?
  options:
  '1': Using sc.textFile()
  '2': Using sc.parallelize()
  '3': Using sc.union()
  '4': Using sc.join()
  answer: '2'
  id: 88

- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: How can you create a distributed dataset from an external file?
  options:
  '1': Using sc.parallelize()
  '2': Using sc.textFile("path")
  '3': Using sc.union()
  '4': Using sc.cartesian()
  answer: '2'
  id: 89

- stack: spark
  type: mcq
  topic: RDD
  difficulty: medium
  question: What is the purpose of the join() operation on distributed datasets?
  options:
  '1': To combine elements with the same key from two RDDs
  '2': To remove duplicates from an RDD
  '3': To split data into multiple partitions
  '4': To merge all RDDs into one without keys
  answer: '1'
  id: 90

- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: What does the union() operation do in distributed datasets?
  options:
  '1': Returns only distinct elements from two RDDs
  '2': Combines two RDDs into one containing all elements
  '3': Performs a cross product of two RDDs
  '4': Groups data by key
  answer: '2'
  id: 91

- stack: spark
  type: mcq
  topic: RDD
  difficulty: easy
  question: What does the distinct() operation do in distributed datasets?
  options:
  '1': Returns only distinct elements from an RDD
  '2': Returns a fixed number of elements
  '3': Joins two datasets together
  '4': Collects elements back to the driver
  answer: '1'
  id: 92

- stack: spark
  type: mcq
  topic: RDD
  difficulty: medium
  question: What does the cartesian() operation do in distributed datasets?
  options:
  '1': Returns a cross product of all elements from two RDDs
  '2': Returns only unique pairs of elements
  '3': Groups values by key
  '4': Returns only matching keys
  answer: '1'
  id: 93

- stack: spark
  type: mcq
  topic: Caching
  difficulty: easy
  question: What is dataset persistence in Spark?
  options:
  '1': The process of recomputing transformations every time
  '2': The mechanism to store RDDs or DataFrames in memory or disk for reuse
  '3': The replication of datasets across all worker nodes
  '4': The automatic saving of data into external databases
  answer: '2'
  id: 94

- stack: spark
  type: mcq
  topic: Caching
  difficulty: medium
  question: What is the difference between cache() and persist() in Spark?
  options:
  '1': cache() stores data in memory only, persist() allows custom storage levels
  '2': cache() saves data to HDFS, persist() keeps it in memory
  '3': persist() is faster than cache() in all cases
  '4': cache() is used for DataFrames only, persist() for RDDs only
  answer: '1'
  id: 95

- stack: spark
  type: mcq
  topic: Caching
  difficulty: medium
  question: What caching strategies does Spark provide?
  options:
  '1': MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, and others
  '2': HDFS_ONLY, DB_ONLY, CACHE_ONLY
  '3': RAM_ONLY, GPU_ONLY, SSD_ONLY
  '4': No caching strategies are provided
  answer: '1'
  id: 96

- stack: spark
  type: mcq
  topic: Caching
  difficulty: medium
  question: Why is caching useful in Spark applications?
  options:
  '1': It reduces recomputation and speeds up iterative algorithms
  '2': It removes the need for data shuffling
  '3': It eliminates the need for cluster managers
  '4': It stores results permanently across Spark sessions
  answer: '1'
  id: 97

- stack: spark
  type: mcq
  topic: Caching
  difficulty: easy
  question: Which method unpersists a cached dataset in Spark?
  options:
  '1': dataset.clear()
  '2': dataset.drop()
  '3': dataset.unpersist()
  '4': dataset.delete()
  answer: '3'
  id: 98

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: What is a DataFrame in Spark?
  options:
  '1': A distributed collection of rows with a schema like a table
  '2': A single file stored in HDFS
  '3': A local array of objects in Scala
  '4': A graph representation of nodes and edges
  answer: '1'
  id: 99

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: medium
  question: What is the difference between an RDD, DataFrame, and Dataset?
  options:
  '1': RDD is low-level, DataFrame adds schema, Dataset provides type safety with schema
  '2': They are all identical in Spark
  '3': DataFrames are faster than Datasets in all cases
  '4': RDDs and Datasets cannot be converted to each other
  answer: '1'
  id: 100

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: medium
  question: What is the role of a schema in Spark DataFrames?
  options:
  '1': It defines data types and column names for structured data
  '2': It stores metadata for RDD partitions
  '3': It specifies how data is replicated
  '4': It controls caching strategies in Spark
  answer: '1'
  id: 101

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: How can you create a DataFrame from an RDD?
  options:
  '1': By using toDF() on the RDD
  '2': By using sc.parallelize()
  '3': By using groupByKey()
  '4': By using reduceByKey()
  answer: '1'
  id: 102

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: How can you create a DataFrame from a JSON file?
  options:
  '1': spark.read.json("path")
  '2': spark.read.csv("path")
  '3': spark.read.parquet("path")
  '4': spark.read.text("path")
  answer: '1'
  id: 103

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: How can you create a DataFrame from a Parquet file?
  options:
  '1': spark.read.json("path")
  '2': spark.read.csv("path")
  '3': spark.read.parquet("path")
  '4': spark.read.text("path")
  answer: '3'
  id: 104

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: How can you create a DataFrame from a CSV file?
  options:
  '1': spark.read.json("path")
  '2': spark.read.csv("path")
  '3': spark.read.text("path")
  '4': spark.read.orc("path")
  answer: '2'
  id: 105

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: medium
  question: How can you create a DataFrame from a Hive table?
  options:
  '1': spark.read.table("table_name")
  '2': spark.read.hdfs("table_name")
  '3': spark.read.json("table_name")
  '4': spark.read.orc("table_name")
  answer: '1'
  id: 106

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: What does the select() operation do in DataFrames?
  options:
  '1': Selects specific columns from a DataFrame
  '2': Filters rows by condition
  '3': Groups data by key
  '4': Saves the DataFrame as a file
  answer: '1'
  id: 107

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: What does withColumn() do in DataFrames?
  options:
  '1': Adds or replaces a column in a DataFrame
  '2': Removes a column from a DataFrame
  '3': Renames the DataFrame schema
  '4': Caches the DataFrame
  answer: '1'
  id: 108

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: easy
  question: What does the filter() operation do in DataFrames?
  options:
  '1': Returns rows that satisfy a condition
  '2': Returns only distinct rows
  '3': Returns the first n rows
  '4': Returns rows grouped by key
  answer: '1'
  id: 109

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: medium
  question: What do groupBy() and agg() do in DataFrames?
  options:
  '1': Group rows by column(s) and apply aggregation functions
  '2': Select specific columns from the dataset
  '3': Add or replace a column
  '4': Filter rows by condition
  answer: '1'
  id: 110

- stack: spark
  type: mcq
  topic: DataFrames
  difficulty: medium
  question: What is partitioning in Spark DataFrames?
  options:
  '1': Dividing DataFrame data across multiple partitions for parallelism
  '2': Saving DataFrames into multiple files
  '3': Removing duplicates from DataFrames
  '4': Splitting DataFrames into smaller DataFrames based on schema
  answer: '1'
  id: 111

- stack: spark
  type: mcq
  topic: Datasets
  difficulty: easy
  question: What is a Dataset in Spark?
  options:
  '1': A distributed collection of strongly-typed objects with schema support
  '2': A distributed collection of raw bytes only
  '3': An unstructured text file stored in HDFS
  '4': A database table stored externally
  answer: '1'
  id: 112

- stack: spark
  type: mcq
  topic: Datasets
  difficulty: medium
  question: What is the main advantage of Datasets over DataFrames?
  options:
  '1': Type safety at compile time while keeping benefits of Spark SQL optimizations
  '2': Faster performance in all cases than DataFrames
  '3': Ability to use Datasets only in Python
  '4': No schema enforcement compared to DataFrames
  answer: '1'
  id: 113

- stack: spark
  type: mcq
  topic: Datasets
  difficulty: medium
  question: Which languages currently support Spark Datasets?
  options:
  '1': Scala and Java
  '2': Python and R
  '3': Only SQL
  '4': All languages supported by Spark
  answer: '1'
  id: 114

- stack: spark
  type: mcq
  topic: Datasets
  difficulty: medium
  question: How can you create a Dataset from a DataFrame in Scala?
  options:
  '1': By using .as[Type] on the DataFrame
  '2': By using sc.parallelize()
  '3': By using groupByKey()
  '4': By using flatMap()
  answer: '1'
  id: 115

- stack: spark
  type: mcq
  topic: Datasets
  difficulty: easy
  question: What is an Encoder in Spark Datasets?
  options:
  '1': A mechanism that maps JVM objects to Spark SQL types for serialization
  '2': A tool for compressing datasets on disk
  '3': A schema validator for Hive queries
  '4': A debugging utility in Spark UI
  answer: '1'
  id: 116

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is the Catalyst Optimizer in Spark?
  options:
  '1': A query optimization framework in Spark SQL
  '2': A caching mechanism for Spark RDDs
  '3': A tool for monitoring Spark jobs
  '4': A library for ML algorithms
  answer: '1'
  id: 117

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: easy
  question: What are broadcast variables in Spark?
  options:
  '1': Variables shared with all worker nodes efficiently without sending with each task
  '2': Variables that accumulate values across nodes
  '3': Variables that represent RDD partitions
  '4': Variables that store shuffle results
  answer: '1'
  id: 118

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: easy
  question: What are accumulators in Spark?
  options:
  '1': Variables used for aggregating values across executors
  '2': Variables that replace broadcast variables
  '3': Variables that store metadata of jobs
  '4': Variables used only in Spark Streaming
  answer: '1'
  id: 119

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is data locality in Spark?
  options:
  '1': Running computations close to the data to reduce network overhead
  '2': Saving data on local disks only
  '3': Partitioning data across executors
  '4': Storing all results on the driver node
  answer: '1'
  id: 120

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: How can partitioning strategies improve Spark performance?
  options:
  '1': By reducing shuffling and aligning data with computation
  '2': By increasing the number of driver nodes
  '3': By removing data locality constraints
  '4': By using only in-memory caching
  answer: '1'
  id: 121

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is shuffling in Spark?
  options:
  '1': The process of redistributing data across partitions for operations like groupBy and join
  '2': The caching of RDDs in memory
  '3': The process of saving RDDs to disk
  '4': The allocation of executors in cluster
  answer: '1'
  id: 122

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: Why is shuffling expensive in Spark?
  options:
  '1': It involves disk I/O, data serialization, and network transfer
  '2': It requires more driver memory
  '3': It eliminates fault tolerance
  '4': It increases the number of DAG stages
  answer: '1'
  id: 123

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: What is the Tungsten execution engine in Spark SQL?
  options:
  '1': A project to improve Spark SQL performance with memory management and code generation
  '2': A library for Spark Streaming
  '3': A caching layer for Spark MLlib
  '4': A UI tool for Spark monitoring
  answer: '1'
  id: 124

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: hard
  question: What is whole-stage code generation in Spark SQL?
  options:
  '1': A technique that generates optimized bytecode for entire query stages
  '2': A method for caching DataFrames
  '3': A way to parallelize file reading
  '4': A driver-level optimization for Spark jobs
  answer: '1'
  id: 125

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: hard
  question: How do broadcast hints improve query performance in Spark SQL?
  options:
  '1': By forcing small tables to be broadcast for efficient joins
  '2': By caching broadcast variables automatically
  '3': By pruning partitions at runtime
  '4': By increasing the number of shuffle partitions
  answer: '1'
  id: 126

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: How does partition pruning optimize queries in Spark SQL?
  options:
  '1': By avoiding reading irrelevant partitions based on filter conditions
  '2': By compressing partition data on disk
  '3': By caching partitions in executors
  '4': By reducing the number of executors
  answer: '1'
  id: 127

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: Why is file format selection important for Spark SQL performance?
  options:
  '1': Columnar formats like Parquet and ORC reduce I/O and improve query speed
  '2': Text formats always run faster than binary formats
  '3': File format has no impact on performance
  '4': Spark only supports CSV and JSON
  answer: '1'
  id: 128

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: How can shuffle performance be optimized in Spark?
  options:
  '1': By tuning spark.sql.shuffle.partitions and using map-side combines
  '2': By increasing driver memory only
  '3': By disabling partitioning
  '4': By using only text file inputs
  answer: '1'
  id: 129

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: Why is partition size important in Spark performance optimization?
  options:
  '1': Too small partitions cause overhead, too large partitions reduce parallelism
  '2': Large partitions always improve performance
  '3': Partition size only affects caching
  '4': Spark ignores partition sizes automatically
  answer: '1'
  id: 130

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: easy
  question: What is the recommended partition size for Spark?
  options:
  '1': 100â€“200 MB per partition
  '2': 10 GB per partition
  '3': 1 KB per partition
  '4': Spark does not recommend partition sizes
  answer: '1'
  id: 131

- stack: spark
  type: mcq
  topic: Optimization
  difficulty: medium
  question: How can broadcast joins improve performance in Spark?
  options:
  '1': By broadcasting small tables to executors and avoiding shuffles
  '2': By caching all large tables automatically
  '3': By pruning partitions dynamically
  '4': By increasing driver parallelism
  answer: '1'
  id: 132

- stack: spark
  type: mcq
  topic: UDF
  difficulty: easy
  question: What is a User-Defined Function (UDF) in Spark?
  options:
  '1': A custom function that extends Spark SQL capabilities
  '2': A built-in aggregation function in Spark
  '3': A method for persisting datasets
  '4': A partitioning strategy in Spark
  answer: '1'
  id: 133

- stack: spark
  type: mcq
  topic: UDF
  difficulty: medium
  question: What is the difference between a UDF and a UDAF in Spark?
  options:
  '1': UDF operates on a single row, UDAF performs aggregations across rows
  '2': UDF is only for Scala, UDAF is for Python
  '3': UDAF is faster than UDF in all cases
  '4': They are identical in Spark SQL
  answer: '1'
  id: 134

- stack: spark
  type: mcq
  topic: UDF
  difficulty: medium
  question: Why can UDFs sometimes reduce Spark SQL performance?
  options:
  '1': They bypass Catalyst Optimizerâ€™s optimizations
  '2': They disable DataFrame caching
  '3': They force Spark to use RDD APIs only
  '4': They increase shuffle partitions automatically
  answer: '1'
  id: 135

- stack: spark
  type: mcq
  topic: Window Functions
  difficulty: easy
  question: What are window functions in Spark SQL?
  options:
  '1': Functions that perform calculations across a group of rows related to the current row
  '2': Functions for filtering DataFrames
  '3': Functions for saving files in windows format
  '4': Functions that repartition data across executors
  answer: '1'
  id: 136

- stack: spark
  type: mcq
  topic: Window Functions
  difficulty: medium
  question: Which of the following is an example of a window function in Spark SQL?
  options:
  '1': row_number()
  '2': groupBy()
  '3': collect()
  '4': repartition()
  answer: '1'
  id: 137

- stack: spark
  type: mcq
  topic: Spark SQL
  difficulty: easy
  question: What is Spark SQL?
  options:
  '1': A module for structured data processing with SQL and DataFrame APIs
  '2': A Spark library for graph processing
  '3': A replacement for Spark Core
  '4': A tool for monitoring Spark applications
  answer: '1'
  id: 138

- stack: spark
  type: mcq
  topic: Spark SQL
  difficulty: medium
  question: What are the main components of Spark SQL?
  options:
  '1': Catalyst Optimizer, Tungsten Execution Engine, SQL Parser
  '2': DAG Scheduler, Shuffle Manager, Cluster Manager
  '3': MLlib, GraphX, Spark Streaming
  '4': Spark Shell, SparkContext, SparkSession
  answer: '1'
  id: 139

- stack: spark
  type: mcq
  topic: Spark SQL
  difficulty: medium
  question: What is schema evolution in Spark SQL?
  options:
  '1': The ability to handle changes in schema over time in data sources
  '2': The process of converting DataFrames to RDDs
  '3': The optimization of SQL queries by Catalyst
  '4': The automatic partitioning of DataFrames
  answer: '1'
  id: 140

- stack: spark
  type: mcq
  topic: Joins
  difficulty: medium
  question: What are the types of joins supported in Spark SQL?
  options:
  '1': Inner, Left, Right, Full, Cross, Semi, Anti
  '2': Only Inner and Outer
  '3': Only Broadcast and Shuffle
  '4': Only Cross joins
  answer: '1'
  id: 141

- stack: spark
  type: mcq
  topic: Joins
  difficulty: medium
  question: What is a broadcast join in Spark SQL?
  options:
  '1': A join where a small table is broadcast to all executors to avoid shuffles
  '2': A join that requires all data to be cached
  '3': A join where results are broadcast to the driver
  '4': A join that only works with JSON data sources
  answer: '1'
  id: 142

- stack: spark
  type: mcq
  topic: Joins
  difficulty: medium
  question: When should broadcast joins be used in Spark?
  options:
  '1': When one table is small enough to fit in executor memory
  '2': When both tables are very large
  '3': Only when using Parquet format
  '4': Only when schema evolution is enabled
  answer: '1'
  id: 143

- stack: spark
  type: mcq
  topic: Joins
  difficulty: medium
  question: What is a shuffle join in Spark SQL?
  options:
  '1': A join that requires shuffling data across the cluster based on join keys
  '2': A join that avoids network transfers
  '3': A join limited to Hive tables
  '4': A join only available in PySpark
  answer: '1'
  id: 144

- stack: spark
  type: mcq
  topic: Joins
  difficulty: medium
  question: What is a sort-merge join in Spark SQL?
  options:
  '1': A join where both datasets are sorted on join keys before merging
  '2': A join that works only with CSV files
  '3': A join used only in local mode
  '4': A join that avoids Catalyst Optimizer
  answer: '1'
  id: 145

- stack: spark
  type: mcq
  topic: Data Skew
  difficulty: easy
  question: What is data skew in Spark?
  options:
  '1': An uneven distribution of data across partitions leading to performance issues
  '2': The compression of data in Parquet format
  '3': The replication of data for fault tolerance
  '4': The caching of data in executors
  answer: '1'
  id: 146

- stack: spark
  type: mcq
  topic: Data Skew
  difficulty: medium
  question: How does Spark SQL handle data skew in joins?
  options:
  '1': By using techniques like broadcast joins, salting, and adaptive execution
  '2': By disabling shuffles
  '3': By repartitioning all tables randomly
  '4': By forcing sort-merge joins only
  answer: '1'
  id: 147

- stack: spark
  type: mcq
  topic: Data Skew
  difficulty: medium
  question: What are techniques to mitigate data skew in Spark?
  options:
  '1': Key salting, broadcast joins, increasing parallelism
  '2': Disabling partitioning, reducing executors
  '3': Converting all tables to CSV format
  '4': Using only RDD APIs
  answer: '1'
  id: 148

- stack: spark
  type: mcq
  topic: Data Skew
  difficulty: medium
  question: What is data skew in Spark?
  options:
  '1': Uneven distribution of data across partitions leading to workload imbalance
  '2': Data loss during shuffling
  '3': Data replication for fault tolerance
  '4': Random partitioning of data regardless of keys
  answer: '1'
  id: 149

- stack: spark
  type: mcq
  topic: Data Skew
  difficulty: medium
  question: How does Spark SQL handle data skew in joins?
  options:
  '1': By using techniques like salting, adaptive execution, or broadcast joins
  '2': By ignoring skewed partitions
  '3': By reducing the number of executors
  '4': By disabling partitioning completely
  answer: '1'
  id: 150

- stack: spark
  type: mcq
  topic: Data Skew
  difficulty: medium
  question: Which of the following is a technique to mitigate data skew in Spark?
  options:
  '1': Key salting
  '2': Adaptive Query Execution (AQE)
  '3': Broadcast joins
  '4': All of the above
  answer: '4'
  id: 151

- stack: spark
  type: mcq
  topic: AQE
  difficulty: medium
  question: What is Adaptive Query Execution (AQE) in Spark SQL?
  options:
  '1': A framework that dynamically optimizes queries at runtime
  '2': A static rule-based optimizer for SQL queries
  '3': A data serialization mechanism
  '4': A checkpointing system for queries
  answer: '1'
  id: 152

- stack: spark
  type: mcq
  topic: AQE
  difficulty: medium
  question: What are the benefits of AQE in Spark SQL?
  options:
  '1': Better join strategies, skew handling, and optimized partition sizes
  '2': Faster Spark job submission
  '3': Automatic caching of DataFrames
  '4': Conversion of RDDs to DataFrames
  answer: '1'
  id: 153

- stack: spark
  type: mcq
  topic: AQE
  difficulty: medium
  question: How does AQE improve join strategies in Spark SQL?
  options:
  '1': By dynamically switching between shuffle and broadcast joins
  '2': By forcing sort-merge joins only
  '3': By removing shuffles entirely
  '4': By running joins only on the driver node
  answer: '1'
  id: 154

- stack: spark
  type: mcq
  topic: AQE
  difficulty: medium
  question: How does AQE handle skewed data in Spark SQL?
  options:
  '1': By splitting skewed partitions into smaller sub-partitions
  '2': By ignoring skewed data
  '3': By storing skewed data in Hive tables
  '4': By converting skewed joins into cartesian joins
  answer: '1'
  id: 155

- stack: spark
  type: mcq
  topic: AQE
  difficulty: medium
  question: Which of the following optimizations can AQE perform?
  options:
  '1': Join strategy optimization
  '2': Skewed partition handling
  '3': Coalescing partitions
  '4': All of the above
  answer: '4'
  id: 156

- stack: spark
  type: mcq
  topic: Partitioning
  difficulty: medium
  question: What is partitioning in Spark SQL?
  options:
  '1': Dividing data into smaller subsets based on column values for faster query performance
  '2': Storing data only on a single node
  '3': Replicating data across executors
  '4': Merging multiple tables into one
  answer: '1'
  id: 157

- stack: spark
  type: mcq
  topic: Partitioning
  difficulty: medium
  question: What is bucketing in Spark SQL?
  options:
  '1': Distributing data into fixed number of buckets based on hash of a column
  '2': Randomly assigning data to partitions
  '3': Replicating data to all executors
  '4': A type of broadcast join strategy
  answer: '1'
  id: 158

- stack: spark
  type: mcq
  topic: Partitioning
  difficulty: medium
  question: What is Z-ordering in Spark SQL?
  options:
  '1': A multi-dimensional clustering technique for improving query performance
  '2': Sorting DataFrames by primary key
  '3': A caching strategy in Spark SQL
  '4': A DAG scheduling technique
  answer: '1'
  id: 159

- stack: spark
  type: mcq
  topic: Partitioning
  difficulty: medium
  question: What is the difference between bucketing and partitioning?
  options:
  '1': Partitioning splits data by column values, bucketing splits data into fixed buckets using a hash function
  '2': Both are identical in Spark SQL
  '3': Bucketing stores data in Hive, partitioning does not
  '4': Partitioning is used only in DataFrames, bucketing only in Datasets
  answer: '1'
  id: 160

- stack: spark
  type: mcq
  topic: Hive
  difficulty: medium
  question: How does Spark SQL integrate with Hive?
  options:
  '1': By reading Hive tables and using Hive metastore for schema management
  '2': By replacing Hiveâ€™s execution engine entirely
  '3': By converting Hive queries into RDD transformations only
  '4': By storing Hive tables in Parquet automatically
  answer: '1'
  id: 161

- stack: spark
  type: mcq
  topic: Hive
  difficulty: medium
  question: What is the Hive metastore in Spark SQL?
  options:
  '1': A central repository storing metadata about Hive tables and schemas
  '2': A cache for Hive queries in Spark
  '3': A file format used by Hive
  '4': A Spark SQL optimizer component
  answer: '1'
  id: 162

- stack: spark
  type: mcq
  topic: Hive
  difficulty: medium
  question: What are managed and external Hive tables in Spark SQL?
  options:
  '1': Managed tables store data in Hive warehouse, external tables reference data outside Hive warehouse
  '2': Both store data only in HDFS
  '3': Managed tables are read-only, external are write-only
  '4': External tables are automatically cached in Spark
  answer: '1'
  id: 163

- stack: spark
  type: mcq
  topic: Hive
  difficulty: medium
  question: How can you enable Hive support in Spark?
  options:
  '1': By using SparkSession.builder.enableHiveSupport()
  '2': By importing HiveContext manually in all jobs
  '3': By installing Hive separately and starting it with Spark
  '4': By configuring Spark to use only JSON data sources
  answer: '1'
  id: 164

- stack: spark
  type: mcq
  topic: DAG Scheduler
  difficulty: medium
  question: What is the role of the DAG Scheduler in Spark?
  options:
  '1': Divides jobs into stages and stages into tasks for execution
  '2': Manages memory allocation for executors
  '3': Handles data serialization in Spark
  '4': Executes SQL queries directly
  answer: '1'
  id: 165

- stack: spark
  type: mcq
  topic: DAG Scheduler
  difficulty: medium
  question: What is the difference between a stage and a task in Sparkâ€™s DAG Scheduler?
  options:
  '1': A stage is a group of tasks, while a task is the smallest execution unit
  '2': A stage runs on a single executor, tasks run across multiple executors
  '3': A stage is a DataFrame, a task is an RDD
  '4': A stage is always larger in memory size than a task
  answer: '1'
  id: 166

- stack: spark
  type: mcq
  topic: DAG Scheduler
  difficulty: medium
  question: What triggers the creation of a new stage in Spark?
  options:
  '1': The presence of a shuffle dependency between operations
  '2': The use of filter transformations
  '3': When an RDD is cached
  '4': When SparkSession is created
  answer: '1'
  id: 167

- stack: spark
  type: mcq
  topic: Task Scheduling
  difficulty: medium
  question: How does Spark assign tasks to executors?
  options:
  '1': Based on data locality and available executor resources
  '2': Randomly without considering data location
  '3': Only by memory size of the executor
  '4': By assigning all tasks to the driver node
  answer: '1'
  id: 168

- stack: spark
  type: mcq
  topic: Shuffle Operations
  difficulty: medium
  question: What is a shuffle in Spark?
  options:
  '1': The process of redistributing data across partitions based on keys
  '2': Saving RDDs to external storage
  '3': Merging multiple DataFrames
  '4': Caching data in executors
  answer: '1'
  id: 169

- stack: spark
  type: mcq
  topic: Shuffle Operations
  difficulty: medium
  question: Why are shuffle operations expensive in Spark?
  options:
  '1': They involve disk I/O, network transfer, and serialization overhead
  '2': They require creating new SparkContexts
  '3': They always need Kryo serialization
  '4': They can only run in cluster mode
  answer: '1'
  id: 170

- stack: spark
  type: mcq
  topic: Tungsten Engine
  difficulty: medium
  question: What is the Tungsten project in Spark?
  options:
  '1': An initiative to improve memory management and CPU efficiency in Spark
  '2': A storage format for Spark SQL
  '3': A shuffle optimization mechanism only
  '4': A garbage collection replacement for Spark
  answer: '1'
  id: 171

- stack: spark
  type: mcq
  topic: Tungsten Engine
  difficulty: medium
  question: What is whole-stage code generation in Tungsten?
  options:
  '1': A technique that generates optimized Java bytecode for query execution
  '2': A caching mechanism for intermediate RDDs
  '3': A way to store DataFrames in Parquet format
  '4': A replacement for Spark DAG Scheduler
  answer: '1'
  id: 172

- stack: spark
  type: mcq
  topic: Tungsten Engine
  difficulty: medium
  question: What is vectorized execution in Spark Tungsten?
  options:
  '1': Processing multiple rows of data at once for efficiency
  '2': Converting RDDs into vectors for MLlib
  '3': A graph computation model in Spark GraphX
  '4': A memory persistence technique in Spark
  answer: '1'
  id: 173

- stack: spark
  type: mcq
  topic: Memory Management
  difficulty: medium
  question: What are the two main categories of memory in Sparkâ€™s unified memory model?
  options:
  '1': Execution memory and storage memory
  '2': Heap memory and off-heap memory
  '3': Driver memory and executor memory
  '4': Cached memory and shuffle memory
  answer: '1'
  id: 174

- stack: spark
  type: mcq
  topic: Memory Management
  difficulty: medium
  question: How does Spark dynamically allocate execution and storage memory?
  options:
  '1': By allowing unused space in one category to be borrowed by the other
  '2': By pre-allocating fixed blocks
  '3': By increasing JVM heap size at runtime
  '4': By creating additional SparkSessions
  answer: '1'
  id: 175

- stack: spark
  type: mcq
  topic: Memory Management
  difficulty: medium
  question: Why is garbage collection tuning important in Spark?
  options:
  '1': To minimize pause times and avoid memory leaks in executors
  '2': To reduce shuffle dependency
  '3': To enable schema evolution
  '4': To optimize Catalyst queries
  answer: '1'
  id: 176

- stack: spark
  type: mcq
  topic: Memory Management
  difficulty: medium
  question: Which JVM garbage collectors are commonly tuned for Spark workloads?
  options:
  '1': G1GC and CMS (Concurrent Mark Sweep)
  '2': Serial GC only
  '3': ZGC exclusively
  '4': None, Spark bypasses JVM GC
  answer: '1'
  id: 177

- stack: spark
  type: mcq
  topic: Serialization
  difficulty: medium
  question: What are the two main serialization formats in Spark?
  options:
  '1': Java serialization and Kryo serialization
  '2': JSON serialization and Avro serialization
  '3': Parquet serialization and ORC serialization
  '4': Protobuf serialization and Thrift serialization
  answer: '1'
  id: 178

- stack: spark
  type: mcq
  topic: Serialization
  difficulty: medium
  question: Why is Kryo serialization preferred over Java serialization in Spark?
  options:
  '1': It is faster and more compact
  '2': It automatically handles schema evolution
  '3': It does not require registering classes
  '4': It eliminates the need for DAG scheduling
  answer: '1'
  id: 179

- stack: spark
  type: mcq
  topic: Serialization
  difficulty: medium
  question: What is a limitation of Kryo serialization?
  options:
  '1': Requires explicit class registration for best performance
  '2': It cannot serialize large objects
  '3': It works only with DataFrames
  '4': It does not support primitive data types
  answer: '1'
  id: 180

- stack: spark
  type: mcq
  topic: Resource Tuning
  difficulty: medium
  question: What are the main executor-related parameters to tune in Spark?
  options:
  '1': Executor memory, number of cores, and number of executors
  '2': Shuffle partitions, SQL parser, and DAG scheduler
  '3': Catalyst optimizer rules and broadcast hints
  '4': SparkSession configurations only
  answer: '1'
  id: 181

- stack: spark
  type: mcq
  topic: Resource Tuning
  difficulty: medium
  question: What is the trade-off in setting a high number of executor cores?
  options:
  '1': Can cause contention and longer garbage collection pauses
  '2': Always guarantees better performance
  '3': Reduces data skew automatically
  '4': Avoids shuffling entirely
  answer: '1'
  id: 182

- stack: spark
  type: mcq
  topic: Resource Tuning
  difficulty: medium
  question: How can improper memory tuning affect Spark performance?
  options:
  '1': It may cause frequent garbage collection and out-of-memory errors
  '2': It always improves Catalyst optimizations
  '3': It reduces DAG scheduling overhead
  '4': It automatically caches all RDDs
  answer: '1'
  id: 183

- stack: spark
  type: mcq
  topic: Resource Tuning
  difficulty: medium
  question: What is dynamic resource allocation in Spark?
  options:
  '1': A feature that scales executors up or down based on workload
  '2': Allocating resources only at application startup
  '3': Manually configuring executor memory and cores
  '4': A way of disabling shuffle
  answer: '1'
  id: 184

- stack: spark
  type: mcq
  topic: Resource Tuning
  difficulty: medium
  question: What is required for dynamic resource allocation to work in Spark?
  options:
  '1': External shuffle service enabled
  '2': Kryo serialization enabled
  '3': Using Catalyst optimizer
  '4': Running only in local mode
  answer: '1'
  id: 185

- stack: spark
  type: mcq
  topic: Cluster Managers
  difficulty: medium
  question: What is the role of a cluster manager in Spark?
  options:
  '1': Allocates resources and manages executors across the cluster
  '2': Parses SQL queries into execution plans
  '3': Handles serialization of objects
  '4': Monitors garbage collection only
  answer: '1'
  id: 186

- stack: spark
  type: mcq
  topic: Cluster Managers
  difficulty: medium
  question: What is the main difference between YARN and Kubernetes as Spark cluster managers?
  options:
  '1': YARN is Hadoop-based, while Kubernetes is container-based
  '2': YARN works only with Scala, Kubernetes only with Python
  '3': Kubernetes is slower than YARN in all cases
  '4': YARN does not support dynamic allocation
  answer: '1'
  id: 187

- stack: spark
  type: mcq
  topic: Cluster Managers
  difficulty: medium
  question: What are the advantages of using Kubernetes over Standalone mode for Spark?
  options:
  '1': Containerization, better scalability, and resource isolation
  '2': Always faster query execution than SQL Catalyst
  '3': No need for executors
  '4': Eliminates the need for shuffle operations
  answer: '1'
  id: 188

- stack: spark
  type: mcq
  topic: Cluster Managers
  difficulty: medium
  question: Why is Mesos less commonly used for Spark today compared to Kubernetes?
  options:
  '1': Decline in community support and preference for Kubernetes features
  '2': Mesos does not support shuffle operations
  '3': Mesos cannot run Spark in cluster mode
  '4': Kubernetes is required for Kryo serialization
  answer: '1'
  id: 189

- stack: spark
  type: mcq
  topic: Debugging
  difficulty: medium
  question: How does the Spark UI help in debugging failed jobs?
  options:
  '1': By showing DAG visualization, stage/task progress, and error logs
  '2': By automatically fixing failed jobs
  '3': By disabling shuffle operations
  '4': By tuning executor memory automatically
  answer: '1'
  id: 190

- stack: spark
  type: mcq
  topic: Logging
  difficulty: medium
  question: Which logging framework does Spark use by default?
  options:
  '1': Log4j
  '2': SLF4J
  '3': Logback
  '4': java.util.logging
  answer: '1'
  id: 191

- stack: spark
  type: mcq
  topic: Logging
  difficulty: medium
  question: Why should sensitive information be excluded from Spark logs?
  options:
  '1': To prevent security risks and data leaks
  '2': To reduce log file size only
  '3': To improve DAG scheduling
  '4': To increase serialization speed
  answer: '1'
  id: 192

- stack: spark
  type: mcq
  topic: Logging
  difficulty: medium
  question: What is a best practice in Spark application logging?
  options:
  '1': Use appropriate log levels (INFO, WARN, ERROR) for clarity
  '2': Log every record processed by executors
  '3': Disable logging completely
  '4': Store logs only in executor memory
  answer: '1'
  id: 193

- stack: spark
  type: mcq
  topic: Deployment
  difficulty: medium
  question: What is AWS EMR in the context of Spark?
  options:
  '1': A managed cluster platform for running Spark and other big data frameworks
  '2': A Spark SQL optimizer
  '3': A data serialization format
  '4': A shuffle service for Spark
  answer: '1'
  id: 194

- stack: spark
  type: mcq
  topic: Deployment
  difficulty: medium
  question: What is Databricks?
  options:
  '1': A managed Spark service with collaborative features and optimizations
  '2': A Spark serialization framework
  '3': A standalone cluster manager
  '4': A JVM garbage collector
  answer: '1'
  id: 195

- stack: spark
  type: mcq
  topic: Deployment
  difficulty: medium
  question: Which of the following is NOT a Spark deployment mode?
  options:
  '1': Local mode
  '2': Cluster mode
  '3': Client mode
  '4': Serverless mode
  answer: '4'
  id: 196

- stack: spark
  type: mcq
  topic: Deployment
  difficulty: medium
  question: What are the advantages of using managed Spark services like Databricks over self-managed clusters?
  options:
  '1': Simplified management, scalability, performance optimizations, and security
  '2': Only cheaper cost
  '3': They eliminate need for partitioning
  '4': They replace Spark SQL Catalyst optimizer
  answer: '1'
  id: 197

- stack: spark
  type: mcq
  topic: Kubernetes
  difficulty: medium
  question: What is Spark on Kubernetes?
  options:
  '1': Running Spark applications in containers orchestrated by Kubernetes
  '2': A Spark serialization framework
  '3': A Spark SQL optimizer
  '4': A storage format for Spark
  answer: '1'
  id: 198

- stack: spark
  type: mcq
  topic: Kubernetes
  difficulty: medium
  question: How is driver scheduling different in Spark on Kubernetes compared to YARN?
  options:
  '1': The driver runs inside a Kubernetes pod instead of YARNâ€™s ApplicationMaster
  '2': The driver always runs on the client machine in Kubernetes
  '3': The driver does not exist in Kubernetes deployments
  '4': Kubernetes requires multiple drivers per job
  answer: '1'
  id: 199

- stack: spark
  type: mcq
  topic: Kubernetes
  difficulty: medium
  question: How are Spark executors launched in Kubernetes?
  options:
  '1': As Kubernetes pods managed by the Kubernetes scheduler
  '2': As threads inside the driver
  '3': As Hadoop containers
  '4': As JVM processes on the client
  answer: '1'
  id: 200

- stack: spark
  type: mcq
  topic: Kubernetes
  difficulty: medium
  question: What challenges arise when running Spark on Kubernetes?
  options:
  '1': Networking, security, resource isolation, and integration complexity
  '2': Lack of shuffle operations
  '3': Inability to use DataFrames
  '4': No support for serialization
  answer: '1'
  id: 201

- stack: spark
  type: mcq
  topic: Kubernetes
  difficulty: medium
  question: How do ConfigMaps and Secrets help Spark on Kubernetes?
  options:
  '1': By providing configuration data and sensitive credentials securely
  '2': By optimizing shuffle performance
  '3': By storing serialized RDDs
  '4': By eliminating garbage collection overhead
  answer: '1'
  id: 202
