- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Big Data?
  answer: Big Data refers to datasets that are too large or complex to be processed by traditional systems, characterized by high Volume, Velocity, and Variety.
  id: 1

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What are the 3Vs of Big Data?
  answer: Volume, Velocity, and Variety.
  id: 2

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is the difference between batch processing and real-time processing?
  answer: Batch processing handles data in bulk at scheduled intervals, while real-time processing handles continuous streams of data with low latency.
  id: 3

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Give an example of a batch processing system.
  answer: Hadoop MapReduce is a batch processing system.
  id: 4

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Give an example of a real-time processing system.
  answer: Apache Kafka with Spark Structured Streaming can be used for real-time processing.
  id: 5

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Apache Spark?
  answer: Apache Spark is an open-source distributed computing framework for fast large-scale data processing across clusters.
  id: 6

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Why is Spark faster than Hadoop MapReduce?
  answer: Because Spark processes data in-memory and minimizes disk I/O, whereas MapReduce stores intermediate data on disk.
  id: 7

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: List the main components of the Spark ecosystem.
  answer: Spark Core, Spark SQL, Spark Streaming, MLlib, and GraphX.
  id: 8

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is Spark Core?
  answer: Spark Core provides basic functionalities like task scheduling, memory management, fault tolerance, and interaction with storage systems.
  id: 9

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Spark SQL used for?
  answer: Spark SQL is used for structured data processing and enables SQL queries on distributed data.
  id: 10

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Spark Streaming?
  answer: Spark Streaming is a component of Spark used for processing real-time streaming data.
  id: 11

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is MLlib in Spark?
  answer: MLlib is Sparkâ€™s scalable machine learning library providing algorithms for classification, regression, clustering, and more.
  id: 12

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is GraphX in Spark?
  answer: GraphX is Sparkâ€™s API for graph-parallel computation and graph analytics.
  id: 13

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Explain Sparkâ€™s architecture in one line.
  answer: Spark follows a master-slave architecture with a Driver program coordinating tasks executed by Executors on cluster nodes.
  id: 14

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is the role of the Driver in Spark?
  answer: The Driver runs the main program, builds the execution plan, and coordinates execution of tasks on Executors.
  id: 15

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What are Executors in Spark?
  answer: Executors are worker processes that run individual tasks and store intermediate data.
  id: 16

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is the role of the Cluster Manager?
  answer: The Cluster Manager allocates resources across applications. Examples are Standalone, YARN, Mesos, and Kubernetes.
  id: 17

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is the difference between Standalone and YARN in Spark?
  answer: Standalone is Sparkâ€™s built-in cluster manager, while YARN is a Hadoop-based cluster manager for resource sharing across applications.
  id: 18

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is Spark on Kubernetes?
  answer: It is Sparkâ€™s native integration with Kubernetes, where Spark applications run as pods in a Kubernetes cluster.
  id: 19

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Local mode in Spark?
  answer: Local mode runs Spark on a single machine for development or testing purposes.
  id: 20

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Cluster mode in Spark?
  answer: Cluster mode runs Spark on multiple nodes managed by a cluster manager for distributed processing.
  id: 21

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Which cluster managers can Spark run on?
  answer: Standalone, Apache YARN, Apache Mesos, and Kubernetes.
  id: 22

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is Spark Shell?
  answer: Spark Shell is an interactive command-line environment available in Scala and Python for experimenting with Spark.
  id: 23

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: Which languages are supported by Spark Shell?
  answer: Scala and Python.
  id: 24

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: Why is Spark Shell useful?
  answer: It allows quick experimentation, testing Spark commands, and interactive data exploration.
  id: 25

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is the difference between PySpark and Scala Spark?
  answer: PySpark provides a Python API for Spark, while Scala is Sparkâ€™s native language and often offers better performance.
  id: 26

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is the role of DAG (Directed Acyclic Graph) in Spark execution?
  answer: The Driver converts user operations into a DAG of stages and tasks, which are then scheduled for execution on Executors.
  id: 27

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is SparkContext?
  answer: SparkContext is the entry point of any Spark application; it connects to the cluster manager and coordinates resources.
  id: 28

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What is SparkSession?
  answer: SparkSession is the unified entry point to work with RDDs, DataFrames, and Datasets in Spark 2.x and above.
  id: 29

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is the difference between SparkContext and SparkSession?
  answer: SparkContext is the entry point for low-level RDD APIs, while SparkSession is a unified entry point introduced in Spark 2.x for all APIs including SQL and DataFrames.
  id: 30

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: What does RDD stand for in Spark and what role does it serve?
  answer: RDD stands for Resilient Distributed Dataset, the core data structure in Spark.
  id: 31

- stack: spark
  type: theory
  topic: Basics
  difficulty: easy
  question: Why are distributed datasets called 'resilient'?
  answer: Because they can automatically recover from node failures using lineage information.
  id: 32

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What does it mean that distributed datasets are immutable?
  answer: Once created, distributed datasets cannot be changed. Any operation produces a new dataset.
  id: 33

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What does lazy evaluation mean in Spark?
  answer: Operations on distributed datasets are not executed immediately; Spark builds a DAG and executes only when an action is triggered.
  id: 34

- stack: spark
  type: theory
  topic: Fault Tolerance
  difficulty: medium
  question: What is fault tolerance in distributed data structures?
  answer: They can recompute lost partitions using lineage information if data is lost due to node failures.
  id: 35

- stack: spark
  type: theory
  topic: Transformations
  difficulty: easy
  question: What are transformations in Spark's distributed datasets?
  answer: Transformations are operations that create a new dataset from an existing one, such as map and filter.
  id: 36

- stack: spark
  type: theory
  topic: Actions
  difficulty: easy
  question: What are actions in Spark's distributed datasets?
  answer: Actions are operations that trigger execution and return results to the driver or write data to storage, such as collect and count.
  id: 37

- stack: spark
  type: theory
  topic: Transformations
  difficulty: medium
  question: Give two examples of transformations in Spark.
  answer: map and filter.
  id: 38

- stack: spark
  type: theory
  topic: Actions
  difficulty: medium
  question: Give two examples of actions in Spark.
  answer: collect and saveAsTextFile.
  id: 39

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is the difference between narrow and wide dependencies in distributed datasets?
  answer: In narrow dependencies, each parent partition is used by at most one child partition. In wide dependencies, multiple child partitions depend on multiple parent partitions, causing shuffles.
  id: 40

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Give an example of a narrow dependency operation.
  answer: map and filter create narrow dependencies.
  id: 41

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: Give an example of a wide dependency operation.
  answer: reduceByKey and groupByKey create wide dependencies because they require shuffling.
  id: 42

- stack: spark
  type: theory
  topic: RDD
  difficulty: easy
  question: How can you create a distributed dataset from a collection?
  answer: By using the parallelize method on a collection.
  id: 43

- stack: spark
  type: theory
  topic: RDD
  difficulty: easy
  question: How can you create a distributed dataset from an external file?
  answer: By using methods like textFile() to read files into distributed datasets.
  id: 44

- stack: spark
  type: theory
  topic: Transformations
  difficulty: easy
  question: What does the map() transformation do on a distributed dataset?
  answer: It applies a function to each element of the dataset and returns a new dataset with the results.
  id: 45

- stack: spark
  type: theory
  topic: Transformations
  difficulty: easy
  question: What does the flatMap() transformation do on a distributed dataset?
  answer: It applies a function to each element and flattens the results into a single dataset.
  id: 46

- stack: spark
  type: theory
  topic: Transformations
  difficulty: easy
  question: What does the filter() transformation do on a distributed dataset?
  answer: It returns a new dataset containing only the elements that satisfy a given condition.
  id: 47

- stack: spark
  type: theory
  topic: Actions
  difficulty: medium
  question: What does reduce() action do in Spark?
  answer: It aggregates the elements of the dataset using a specified associative function.
  id: 48

- stack: spark
  type: theory
  topic: Actions
  difficulty: medium
  question: What is the difference between reduceByKey and groupByKey?
  answer: reduceByKey combines values for each key locally before shuffling, while groupByKey shuffles all key-value pairs, making it less efficient.
  id: 49

- stack: spark
  type: theory
  topic: RDD
  difficulty: medium
  question: What is the purpose of the join() operation on distributed datasets?
  answer: It combines two datasets based on matching keys, similar to SQL joins.
  id: 50

- stack: spark
  type: theory
  topic: RDD
  difficulty: medium
  question: What does the union() operation do in distributed datasets?
  answer: It returns a dataset that contains elements from both source datasets.
  id: 51

- stack: spark
  type: theory
  topic: RDD
  difficulty: medium
  question: What does the distinct() operation do in distributed datasets?
  answer: It returns a dataset with duplicate elements removed.
  id: 52

- stack: spark
  type: theory
  topic: RDD
  difficulty: medium
  question: What does the cartesian() operation do in distributed datasets?
  answer: It returns all possible pairs between elements of two datasets, which can be very expensive.
  id: 53

- stack: spark
  type: theory
  topic: Actions
  difficulty: easy
  question: What does the collect() action do on a distributed dataset?
  answer: It returns all elements of the dataset to the driver program.
  id: 54

- stack: spark
  type: theory
  topic: Actions
  difficulty: easy
  question: What does the count() action do on a distributed dataset?
  answer: It returns the number of elements in the dataset.
  id: 55

- stack: spark
  type: theory
  topic: Actions
  difficulty: easy
  question: What does the take() action do on a distributed dataset?
  answer: It returns the first N elements of the dataset.
  id: 56

- stack: spark
  type: theory
  topic: Actions
  difficulty: medium
  question: What does saveAsTextFile() action do?
  answer: It saves the elements of a dataset as a text file in the specified directory.
  id: 57

- stack: spark
  type: theory
  topic: Caching
  difficulty: medium
  question: What is dataset persistence?
  answer: Persistence allows intermediate datasets to be stored in memory or on disk for reuse in future computations.
  id: 58

- stack: spark
  type: theory
  topic: Caching
  difficulty: medium
  question: What is the difference between cache() and persist() in Spark?
  answer: cache() stores datasets in memory only, while persist() allows specifying different storage levels such as memory and disk.
  id: 59

- stack: spark
  type: theory
  topic: Basics
  difficulty: medium
  question: What is lineage in distributed datasets?
  answer: Lineage is the sequence of transformations used to build a dataset, which Spark uses to recompute lost data partitions.
  id: 60

- stack: spark
  type: theory
  topic: Basics
  difficulty: hard
  question: How does Spark use DAG (Directed Acyclic Graph) for distributed datasets?
  answer: Spark represents transformations as a DAG of stages and tasks. The DAG scheduler optimizes and executes this plan efficiently across the cluster.
  id: 61

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: easy
  question: What is a DataFrame in Spark?
  answer: A DataFrame is a distributed collection of data organized into named columns, similar to a table in a relational database.
  id: 62

- stack: spark
  type: theory
  topic: Datasets
  difficulty: easy
  question: What is a Dataset in Spark?
  answer: A Dataset is a strongly-typed distributed collection of data introduced in Spark 1.6, combining the benefits of RDDs and DataFrames.
  id: 63

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: What is the difference between an RDD, DataFrame, and Dataset?
  answer: RDDs provide low-level control with no schema, DataFrames are schema-based and optimized, and Datasets add compile-time type safety on top of DataFrames.
  id: 64

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: What is the role of a schema in Spark DataFrames?
  answer: A schema defines the structure of the DataFrame including column names and data types, enabling optimized execution through Catalyst.
  id: 65

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: What is the Catalyst Optimizer?
  answer: Catalyst Optimizer is Spark SQLâ€™s query optimization engine that automatically optimizes logical query plans into efficient physical execution plans.
  id: 66

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: easy
  question: How can you create a DataFrame from an RDD?
  answer: By applying a schema using SparkSession.createDataFrame() on an RDD.
  id: 67

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: easy
  question: How can you create a DataFrame from a JSON file?
  answer: By using spark.read.json('path').
  id: 68

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: easy
  question: How can you create a DataFrame from a Parquet file?
  answer: By using spark.read.parquet('path').
  id: 69

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: easy
  question: How can you create a DataFrame from a CSV file?
  answer: By using spark.read.csv('path', header=True, inferSchema=True).
  id: 70

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: How can you create a DataFrame from a Hive table?
  answer: By enabling Hive support and using spark.sql('SELECT * FROM table_name').
  id: 71

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: What does the select() operation do in DataFrames?
  answer: select() returns a new DataFrame with specified columns.
  id: 72

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: What does withColumn() do in DataFrames?
  answer: withColumn() creates a new column or replaces an existing one with a computed value.
  id: 73

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: easy
  question: What does the filter() operation do in DataFrames?
  answer: filter() returns rows that satisfy a given condition.
  id: 74

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: What does groupBy() and agg() do in DataFrames?
  answer: groupBy() groups rows by column values and agg() performs aggregations such as sum, avg, or count.
  id: 75

- stack: spark
  type: theory
  topic: SQL
  difficulty: medium
  question: How can SQL queries be executed in Spark?
  answer: By registering DataFrames as temporary views and running queries using spark.sql().
  id: 76

- stack: spark
  type: theory
  topic: Data Sources
  difficulty: easy
  question: What is the advantage of using Parquet as a data source?
  answer: Parquet is a columnar storage format that enables efficient compression and query performance.
  id: 77

- stack: spark
  type: theory
  topic: Data Sources
  difficulty: medium
  question: What is the difference between Parquet and JSON as Spark data sources?
  answer: Parquet is columnar, compact, and optimized for queries, while JSON is row-based and more human-readable but less efficient.
  id: 78

- stack: spark
  type: theory
  topic: Data Sources
  difficulty: medium
  question: How can Spark connect to relational databases?
  answer: By using the JDBC data source with spark.read.format('jdbc').
  id: 79

- stack: spark
  type: theory
  topic: UDF
  difficulty: medium
  question: What is a User-Defined Function (UDF) in Spark?
  answer: A UDF is a custom function defined by users to extend the functionality of Spark SQL.
  id: 80

- stack: spark
  type: theory
  topic: UDF
  difficulty: medium
  question: What is the difference between a UDF and a UDAF?
  answer: A UDF operates on individual rows, while a UDAF (User-Defined Aggregate Function) performs aggregation over multiple rows.
  id: 81

- stack: spark
  type: theory
  topic: Window Functions
  difficulty: hard
  question: What are window functions in Spark SQL?
  answer: Window functions perform calculations across a set of rows related to the current row, such as ranking and moving averages.
  id: 82

- stack: spark
  type: theory
  topic: Window Functions
  difficulty: medium
  question: Give an example of a window function in Spark.
  answer: ROW_NUMBER() OVER (PARTITION BY col ORDER BY col2) assigns sequential numbers within each partition.
  id: 83

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: What are broadcast variables in Spark?
  answer: Broadcast variables allow large read-only data to be cached on worker nodes, reducing communication overhead.
  id: 84

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: What are accumulators in Spark?
  answer: Accumulators are variables that workers can only add to, useful for counters and aggregations.
  id: 85

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: What is data locality in Spark?
  answer: Data locality refers to executing tasks as close as possible to where the data resides to reduce network overhead.
  id: 86

- stack: spark
  type: theory
  topic: Dataframes
  difficulty: medium
  question: What is partitioning in Spark DataFrames?
  answer: Partitioning splits data into smaller chunks stored across nodes, enabling parallelism and optimized processing.
  id: 87

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: How can partitioning strategies improve Spark performance?
  answer: By colocating related data in the same partition, reducing shuffles and improving query execution.
  id: 88

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: What is shuffling in Spark?
  answer: Shuffling is the process of redistributing data across partitions, usually caused by operations like groupBy, reduceByKey, and join.
  id: 89

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: Why is shuffling expensive in Spark?
  answer: Because it involves disk I/O, network transfer, and serialization, making it one of the most costly operations in Spark.
  id: 90

- stack: spark
  type: theory
  topic: Spark SQL
  difficulty: medium
  question: What is Spark SQL?
  answer: Spark SQL is a Spark module for structured data processing, allowing SQL queries alongside DataFrame and Dataset APIs.
  id: 91

- stack: spark
  type: theory
  topic: Spark SQL
  difficulty: medium
  question: What are the main components of Spark SQL?
  answer: Spark SQL consists of the Catalyst Optimizer for query optimization and the Tungsten execution engine for efficient physical execution.
  id: 92

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: What is the role of the Catalyst Optimizer in Spark SQL?
  answer: The Catalyst Optimizer transforms logical query plans into optimized physical execution plans using rule-based and cost-based optimization.
  id: 93

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: What is Tungsten execution engine in Spark SQL?
  answer: Tungsten is a Spark SQL execution engine designed for memory management, code generation, and CPU efficiency.
  id: 94

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: What is whole-stage code generation in Spark SQL?
  answer: Whole-stage code generation compiles multiple operators into a single optimized Java function to reduce overhead and improve performance.
  id: 95

- stack: spark
  type: theory
  topic: Spark SQL
  difficulty: medium
  question: What is schema evolution in Spark SQL?
  answer: Schema evolution is the ability of Spark SQL to handle changes in schema, such as adding new columns, without breaking queries.
  id: 96

- stack: spark
  type: theory
  topic: Joins
  difficulty: easy
  question: What are the types of joins supported in Spark SQL?
  answer: Spark SQL supports inner, left, right, full outer, semi, anti, cross, broadcast, shuffle, and sort-merge joins.
  id: 97

- stack: spark
  type: theory
  topic: Joins
  difficulty: medium
  question: What is a broadcast join in Spark SQL?
  answer: A broadcast join sends a small table to all worker nodes to avoid shuffling the larger table.
  id: 98

- stack: spark
  type: theory
  topic: Joins
  difficulty: medium
  question: When should broadcast joins be used in Spark?
  answer: Broadcast joins are efficient when one dataset is small enough to fit in memory and can be broadcasted to all nodes.
  id: 99

- stack: spark
  type: theory
  topic: Joins
  difficulty: medium
  question: What is a shuffle join in Spark SQL?
  answer: A shuffle join redistributes data across partitions so that matching keys from both datasets end up on the same node.
  id: 100

- stack: spark
  type: theory
  topic: Joins
  difficulty: hard
  question: What is a sort-merge join in Spark SQL?
  answer: Sort-merge join sorts both datasets on join keys and then merges them, suitable for large datasets.
  id: 101

- stack: spark
  type: theory
  topic: Data Skew
  difficulty: hard
  question: How does Spark SQL handle data skew in joins?
  answer: Spark SQL uses techniques such as salting, skew join optimization, and broadcasting small partitions to handle skew.
  id: 102

- stack: spark
  type: theory
  topic: AQE
  difficulty: medium
  question: What is Adaptive Query Execution (AQE) in Spark SQL?
  answer: AQE dynamically optimizes query plans at runtime based on statistics collected during execution.
  id: 103

- stack: spark
  type: theory
  topic: AQE
  difficulty: medium
  question: What are the benefits of AQE in Spark SQL?
  answer: AQE improves join strategies, reduces shuffle partitions, and handles skewed data dynamically.
  id: 104

- stack: spark
  type: theory
  topic: AQE
  difficulty: hard
  question: How does AQE improve join strategies in Spark SQL?
  answer: AQE can switch from a sort-merge join to a broadcast join at runtime if it detects a small dataset.
  id: 105

- stack: spark
  type: theory
  topic: AQE
  difficulty: hard
  question: How does AQE handle skewed data in Spark SQL?
  answer: AQE splits skewed partitions into smaller sub-partitions to balance the workload.
  id: 106

- stack: spark
  type: theory
  topic: Partitioning
  difficulty: medium
  question: What is bucketing in Spark SQL?
  answer: Bucketing distributes rows into a fixed number of buckets based on the hash of a column, improving join performance.
  id: 107

- stack: spark
  type: theory
  topic: Partitioning
  difficulty: medium
  question: What is partitioning in Spark SQL?
  answer: Partitioning organizes data into directories based on column values, enabling efficient query pruning.
  id: 108

- stack: spark
  type: theory
  topic: Partitioning
  difficulty: hard
  question: What is Z-ordering in Spark SQL?
  answer: Z-ordering is a data layout technique that colocates related information in the same file blocks to improve query performance.
  id: 109

- stack: spark
  type: theory
  topic: Partitioning
  difficulty: medium
  question: What is the difference between bucketing and partitioning?
  answer: Partitioning divides data into directories by column values, while bucketing distributes data into a fixed number of files by hashing column values.
  id: 110

- stack: spark
  type: theory
  topic: Hive
  difficulty: medium
  question: How does Spark SQL integrate with Hive?
  answer: Spark SQL can use Hive metastore, run HiveQL queries, and read/write Hive tables using Hive support.
  id: 111

- stack: spark
  type: theory
  topic: Hive
  difficulty: medium
  question: What is the Hive metastore in Spark SQL?
  answer: The Hive metastore is a central repository of metadata for Hive and Spark SQL tables.
  id: 112

- stack: spark
  type: theory
  topic: Hive
  difficulty: medium
  question: What are managed and external Hive tables in Spark SQL?
  answer: Managed tables are controlled fully by Spark/Hive, while external tables only store metadata in Hive and data outside.
  id: 113

- stack: spark
  type: theory
  topic: Hive
  difficulty: hard
  question: How can you enable Hive support in Spark?
  answer: By creating a SparkSession with enableHiveSupport() method.
  id: 114

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: What are some query tuning techniques in Spark SQL?
  answer: Techniques include caching, broadcast hints, partition pruning, reducing shuffles, and using optimized file formats like Parquet.
  id: 115

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: How do broadcast hints improve query performance in Spark SQL?
  answer: Broadcast hints force Spark to broadcast a table, avoiding shuffle joins for small datasets.
  id: 116

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: How does partition pruning optimize queries in Spark SQL?
  answer: Partition pruning skips scanning irrelevant partitions based on query filters.
  id: 117

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: Why is file format selection important for Spark SQL performance?
  answer: Columnar formats like Parquet and ORC improve compression and query efficiency compared to row-based formats like JSON or CSV.
  id: 118

- stack: spark
  type: theory
  topic: DAG Scheduler
  difficulty: easy
  question: What is the role of the DAG Scheduler in Spark?
  answer: The DAG Scheduler translates a logical execution plan into stages and tasks, scheduling them for execution across cluster nodes.
  id: 119

- stack: spark
  type: theory
  topic: DAG Scheduler
  difficulty: medium
  question: What is the difference between a stage and a task in Sparkâ€™s DAG Scheduler?
  answer: A stage is a set of tasks that can be executed in parallel, while a task is the smallest unit of work sent to a single executor.
  id: 120

- stack: spark
  type: theory
  topic: DAG Scheduler
  difficulty: medium
  question: What triggers the creation of a new stage in Spark?
  answer: A new stage is created when a wide dependency (shuffle operation) is encountered.
  id: 121

- stack: spark
  type: theory
  topic: Task Scheduling
  difficulty: medium
  question: How does Spark assign tasks to executors?
  answer: The Task Scheduler assigns tasks to executors based on data locality and available resources.
  id: 122

- stack: spark
  type: theory
  topic: shuffle operations
  difficulty: easy
  question: What is a shuffle in Spark?
  answer: A shuffle is the process of redistributing data across partitions, typically caused by wide transformations such as reduceByKey or join.
  id: 123

- stack: spark
  type: theory
  topic: shuffle operations
  difficulty: medium
  question: Why are shuffle operations expensive in Spark?
  answer: Because they involve disk I/O, network data transfer, and serialization/deserialization overhead.
  id: 124

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: How can shuffle performance be optimized in Spark?
  answer: By using operations like reduceByKey instead of groupByKey, tuning partition sizes, and enabling map-side combine.
  id: 125

- stack: spark
  type: theory
  topic: Tungsten Engine
  difficulty: easy
  question: What is the Tungsten project in Spark?
  answer: Tungsten is a Spark initiative to improve performance using memory management, whole-stage code generation, and vectorized execution.
  id: 126

- stack: spark
  type: theory
  topic: Tungsten Engine
  difficulty: medium
  question: What is whole-stage code generation in Tungsten?
  answer: Whole-stage code generation compiles multiple operators into optimized Java bytecode to reduce CPU overhead.
  id: 127

- stack: spark
  type: theory
  topic: Tungsten Engine
  difficulty: medium
  question: What is vectorized execution in Spark Tungsten?
  answer: Vectorized execution processes multiple rows in a single CPU instruction batch, improving performance for columnar formats like Parquet.
  id: 128

- stack: spark
  type: theory
  topic: Memory Management
  difficulty: easy
  question: What are the two main categories of memory in Sparkâ€™s unified memory model?
  answer: Execution memory (used for shuffles, joins, sorts) and storage memory (used for caching and broadcast variables).
  id: 129

- stack: spark
  type: theory
  topic: Memory Management
  difficulty: medium
  question: How does Spark dynamically allocate execution and storage memory?
  answer: Spark allows execution and storage memory to borrow space from each other, as long as tasks or cached data can be safely evicted.
  id: 130

- stack: spark
  type: theory
  topic: Caching
  difficulty: medium
  question: What caching strategies does Spark provide?
  answer: Spark provides cache() for in-memory storage and persist() for specifying storage levels like memory-only, memory-and-disk, or disk-only.
  id: 131

- stack: spark
  type: theory
  topic: Memory Management
  difficulty: medium
  question: Why is garbage collection tuning important in Spark?
  answer: Poor GC tuning can cause long pause times, out-of-memory errors, and performance bottlenecks in Spark jobs.
  id: 132

- stack: spark
  type: theory
  topic: Memory Management
  difficulty: hard
  question: Which JVM garbage collectors are commonly tuned for Spark workloads?
  answer: G1GC and CMS (Concurrent Mark-Sweep) are commonly tuned for Spark workloads.
  id: 133

- stack: spark
  type: theory
  topic: Serialization
  difficulty: easy
  question: What are the two main serialization formats in Spark?
  answer: Java serialization and Kryo serialization.
  id: 134

- stack: spark
  type: theory
  topic: Serialization
  difficulty: medium
  question: Why is Kryo serialization preferred over Java serialization in Spark?
  answer: Kryo is faster and produces smaller serialized objects compared to Java serialization.
  id: 135

- stack: spark
  type: theory
  topic: Serialization
  difficulty: medium
  question: What is a limitation of Kryo serialization?
  answer: You may need to register custom classes manually to achieve optimal performance.
  id: 136

- stack: spark
  type: theory
  topic: Data Skew
  difficulty: medium
  question: What is data skew in Spark?
  answer: Data skew occurs when certain partitions receive disproportionately large amounts of data, causing performance bottlenecks.
  id: 137

- stack: spark
  type: theory
  topic: Data Skew
  difficulty: hard
  question: What are techniques to mitigate data skew in Spark?
  answer: Using salting, custom partitioning, broadcast joins, or increasing parallelism.
  id: 138

- stack: spark
  type: theory
  topic: Resource Tuning
  difficulty: easy
  question: What are the main executor-related parameters to tune in Spark?
  answer: Number of executors, executor cores, and executor memory.
  id: 139

- stack: spark
  type: theory
  topic: Resource Tuning
  difficulty: medium
  question: What is the trade-off in setting a high number of executor cores?
  answer: More cores increase parallelism but may cause garbage collection overhead and reduce data locality.
  id: 140

- stack: spark
  type: theory
  topic: Resource Tuning
  difficulty: medium
  question: How can improper memory tuning affect Spark performance?
  answer: Too little memory causes frequent spills to disk, while too much memory may lead to long GC pauses.
  id: 141

- stack: spark
  type: theory
  topic: AQE
  difficulty: easy
  question: What is Adaptive Query Execution (AQE) in Spark?
  answer: AQE is a Spark feature that dynamically optimizes query plans at runtime based on actual data statistics.
  id: 142

- stack: spark
  type: theory
  topic: AQE
  difficulty: medium
  question: What kind of optimizations can AQE perform?
  answer: Optimizations include dynamically switching join strategies, optimizing shuffle partitions, and handling skewed data.
  id: 143

- stack: spark
  type: theory
  topic: AQE
  difficulty: hard
  question: How does AQE improve skew handling in Spark SQL?
  answer: AQE can split skewed partitions into smaller sub-partitions and balance workloads across executors.
  id: 144

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: Why is partition size important in Spark performance optimization?
  answer: Too few partitions cause underutilization of resources, while too many partitions increase scheduling overhead.
  id: 145

- stack: spark
  type: theory
  topic: Optimization
  difficulty: medium
  question: What is the recommended partition size for Spark?
  answer: Typically between 128MB and 256MB per partition for efficient processing.
  id: 146

- stack: spark
  type: theory
  topic: Optimization
  difficulty: hard
  question: How can broadcast joins improve performance in Spark?
  answer: By broadcasting small tables to all executors, broadcast joins eliminate the need for shuffling large datasets.
  id: 147

- stack: spark
  type: theory
  topic: Cluster Managers
  difficulty: easy
  question: What is the role of a cluster manager in Spark?
  answer: A cluster manager allocates resources such as CPU and memory across Spark applications and coordinates execution.
  id: 148

- stack: spark
  type: theory
  topic: Cluster Managers
  difficulty: medium
  question: What is the main difference between YARN and Kubernetes as Spark cluster managers?
  answer: YARN is a Hadoop-based resource manager, while Kubernetes is a container-orchestration system that runs Spark applications as pods.
  id: 149

- stack: spark
  type: theory
  topic: Cluster Managers
  difficulty: medium
  question: What are the advantages of using Kubernetes over Standalone mode for Spark?
  answer: Kubernetes offers containerized deployment, better isolation, scalability, and integration with cloud-native tools.
  id: 150

- stack: spark
  type: theory
  topic: Cluster Managers
  difficulty: hard
  question: Why is Mesos less commonly used for Spark today compared to Kubernetes?
  answer: Kubernetes gained popularity due to strong community adoption, cloud integration, and flexibility, while Mesos adoption declined.
  id: 151

- stack: spark
  type: theory
  topic: Resource Tuning
  difficulty: easy
  question: What is dynamic resource allocation in Spark?
  answer: Dynamic resource allocation allows Spark to add or remove executors at runtime based on workload requirements.
  id: 152

- stack: spark
  type: theory
  topic: Resource Tuning
  difficulty: medium
  question: What is required for dynamic resource allocation to work in Spark?
  answer: It requires an external shuffle service to preserve shuffle files when executors are removed.
  id: 153

- stack: spark
  type: theory
  topic: Checkpointing
  difficulty: easy
  question: What is checkpointing in Spark?
  answer: Checkpointing saves RDD or streaming state to reliable storage like HDFS to provide fault tolerance.
  id: 154

- stack: spark
  type: theory
  topic: Checkpointing
  difficulty: medium
  question: How is checkpointing different from caching in Spark?
  answer: Caching stores data in memory/disk for faster access, while checkpointing saves data to reliable storage for fault recovery.
  id: 155

- stack: spark
  type: theory
  topic: Fault Tolerance
  difficulty: medium
  question: How does Spark achieve fault tolerance in Structured Streaming?
  answer: It uses checkpoints and write-ahead logs (WAL) to recover from failures and resume processing without data loss.
  id: 156

- stack: spark
  type: theory
  topic: Delta Lake
  difficulty: easy
  question: What is Delta Lake in Spark?
  answer: Delta Lake is a storage layer that brings ACID transactions, schema enforcement, and time travel to Spark data lakes.
  id: 157

- stack: spark
  type: theory
  topic: Apache Hudi
  difficulty: easy
  question: What is Apache Hudi?
  answer: Apache Hudi is a data lake framework enabling incremental data processing, upserts, and time-travel queries.
  id: 158

- stack: spark
  type: theory
  topic: Apache Iceberg
  difficulty: easy
  question: What is Apache Iceberg?
  answer: Apache Iceberg is a high-performance table format for huge analytic datasets supporting schema evolution and partition evolution.
  id: 159

- stack: spark
  type: theory
  topic: Lake House
  difficulty: medium
  question: How does Delta Lake differ from Apache Iceberg?
  answer: Delta Lake emphasizes ACID transactions and streaming integration, while Iceberg focuses on flexible schema and partition evolution.
  id: 160

- stack: spark
  type: theory
  topic: Structured Streaming
  difficulty: medium
  question: How does Spark Structured Streaming guarantee exactly-once processing?
  answer: By combining idempotent sinks, checkpointing, and write-ahead logs to ensure no duplicate or lost records.
  id: 161

- stack: spark
  type: theory
  topic: Structured Streaming
  difficulty: hard
  question: What challenges exist in achieving exactly-once guarantees in Spark Streaming?
  answer: Challenges include handling retries, ensuring sink idempotency, and maintaining consistent offsets across sources.
  id: 162

- stack: spark
  type: theory
  topic: Spark Security
  difficulty: easy
  question: What authentication mechanism is commonly used in Spark on Hadoop clusters?
  answer: Kerberos is commonly used for authentication in Spark on Hadoop clusters.
  id: 163

- stack: spark
  type: theory
  topic: Spark Security
  difficulty: medium
  question: How does Spark provide data encryption?
  answer: Spark supports encryption for data at rest and in transit, using SSL for communication and encryption keys for storage.
  id: 164

- stack: spark
  type: theory
  topic: Spark Security
  difficulty: hard
  question: What are the main challenges of implementing security in multi-tenant Spark clusters?
  answer: Challenges include isolation of workloads, secure authentication, role-based authorization, and protecting sensitive logs.
  id: 165

- stack: spark
  type: theory
  topic: Multi-Tenancy
  difficulty: medium
  question: What is multi-tenancy in Spark clusters?
  answer: Multi-tenancy refers to multiple users or teams sharing the same Spark cluster with proper resource isolation and fairness.
  id: 166

- stack: spark
  type: theory
  topic: Multi-Tenancy
  difficulty: medium
  question: Which Spark features help enable multi-tenancy?
  answer: Features like YARN queues, Kubernetes namespaces, and Spark fair scheduler help enable multi-tenancy.
  id: 167

- stack: spark
  type: theory
  topic: Monitoring
  difficulty: easy
  question: What is the Spark UI used for?
  answer: The Spark UI provides a web interface to monitor jobs, stages, tasks, storage, and environment details.
  id: 168

- stack: spark
  type: theory
  topic: Monitoring
  difficulty: medium
  question: How can Spark be integrated with Prometheus and Grafana?
  answer: By exposing Spark metrics via JMX or Prometheus exporters and visualizing them on Grafana dashboards.
  id: 169

- stack: spark
  type: theory
  topic: Monitoring
  difficulty: medium
  question: What metrics are commonly monitored in Spark applications?
  answer: Metrics include job duration, task failures, shuffle read/write size, memory usage, and executor utilization.
  id: 170

- stack: spark
  type: theory
  topic: Debugging
  difficulty: medium
  question: How does the Spark UI help in debugging failed jobs?
  answer: It shows failed tasks, error logs, stage execution details, and skewed partition information for troubleshooting.
  id: 171

- stack: spark
  type: theory
  topic: Logging
  difficulty: easy
  question: Which logging framework does Spark use by default?
  answer: Spark uses Log4j as its default logging framework.
  id: 172

- stack: spark
  type: theory
  topic: Logging
  difficulty: medium
  question: What are logging best practices in Spark applications?
  answer: Use appropriate log levels, avoid excessive logging, externalize log configuration, and centralize logs using tools like ELK or Splunk.
  id: 173

- stack: spark
  type: theory
  topic: Logging
  difficulty: hard
  question: Why should sensitive information be excluded from Spark logs?
  answer: Because logs may be shared across teams or stored centrally, leaking sensitive data like credentials or PII poses security risks.
  id: 174

- stack: spark
  type: theory
  topic: Deployment
  difficulty: easy
  question: What is AWS EMR in the context of Spark?
  answer: Amazon EMR is a managed Hadoop and Spark service on AWS that simplifies cluster provisioning and scaling.
  id: 175

- stack: spark
  type: theory
  topic: Deployment
  difficulty: easy
  question: What is Databricks?
  answer: Databricks is a managed cloud platform built on Apache Spark, offering collaborative notebooks, optimized runtime, and ML integration.
  id: 176

- stack: spark
  type: theory
  topic: Deployment
  difficulty: medium
  question: What is GCP Dataproc?
  answer: Google Cloud Dataproc is a managed service for running Spark and Hadoop clusters on Google Cloud.
  id: 177

- stack: spark
  type: theory
  topic: Deployment
  difficulty: medium
  question: What is Azure HDInsight?
  answer: Azure HDInsight is a managed cloud service for running Apache Spark, Hadoop, and other big data frameworks on Azure.
  id: 178

- stack: spark
  type: theory
  topic: Deployment
  difficulty: hard
  question: What are the advantages of using managed Spark services like Databricks over self-managed clusters?
  answer: Advantages include simplified management, auto-scaling, performance optimizations, built-in security, and better integration with cloud ecosystems.
  id: 179

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: easy
  question: What is Spark on Kubernetes?
  answer: It is the native integration of Apache Spark with Kubernetes, where Spark applications run as Kubernetes pods.
  id: 180

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: medium
  question: How is driver scheduling different in Spark on Kubernetes compared to YARN?
  answer: In Kubernetes, the driver runs inside a pod, while in YARN the driver typically runs on the cluster node managed by YARN.
  id: 181

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: medium
  question: How are Spark executors launched in Kubernetes?
  answer: Executors are created as separate pods by the Kubernetes scheduler, based on Spark resource requests.
  id: 182

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: medium
  question: What is the role of the Kubernetes scheduler in Spark?
  answer: It handles pod placement, resource allocation, and executor scaling for Spark workloads.
  id: 183

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: hard
  question: What challenges arise when running Spark on Kubernetes?
  answer: Challenges include networking setup, persistent storage integration, dynamic scaling, and security configurations.
  id: 184

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: hard
  question: How does Spark on Kubernetes handle dynamic resource allocation?
  answer: It integrates with Kubernetesâ€™ pod lifecycle and requires external shuffle service or Kubernetes-native shuffle implementations.
  id: 185

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: hard
  question: What are the advantages of using Spark on Kubernetes over YARN?
  answer: Advantages include containerization, cloud-native deployment, better isolation, and compatibility with DevOps tooling.
  id: 186

- stack: spark
  type: theory
  topic: Kubernetes
  difficulty: medium
  question: How do ConfigMaps and Secrets help Spark on Kubernetes?
  answer: They allow injecting configuration files and secure credentials into Spark pods.
  id: 187

- stack: spark
  type: theory
  topic: Lake House
  difficulty: easy
  question: What is a Lakehouse in the context of Spark?
  answer: A Lakehouse is a data architecture combining features of data lakes and data warehouses, often powered by formats like Delta Lake, Iceberg, and Hudi.
  id: 188

- stack: spark
  type: theory
  topic: Lake House
  difficulty: medium
  question: How does a Lakehouse differ from a traditional data warehouse?
  answer: Lakehouses support raw data storage and schema-on-read, while also providing ACID transactions and schema enforcement like warehouses.
  id: 189

- stack: spark
  type: theory
  topic: Delta Lake
  difficulty: easy
  question: What are Delta Lake transaction logs?
  answer: They are JSON-based logs that track all changes to a Delta table, enabling ACID transactions and time travel.
  id: 190

- stack: spark
  type: theory
  topic: Delta Lake
  difficulty: medium
  question: How does Delta Lake support schema enforcement?
  answer: It validates incoming data against the table schema and rejects incompatible records.
  id: 191

- stack: spark
  type: theory
  topic: Delta Lake
  difficulty: hard
  question: What is Delta Lake Z-Ordering?
  answer: It is a multi-dimensional clustering technique to optimize query performance by colocating related records.
  id: 192

- stack: spark
  type: theory
  topic: Apache Iceberg
  difficulty: easy
  question: How does Apache Iceberg handle schema evolution?
  answer: Iceberg allows adding, dropping, and renaming columns without rewriting the entire dataset.
  id: 193

- stack: spark
  type: theory
  topic: Apache Iceberg
  difficulty: medium
  question: What is hidden partitioning in Iceberg?
  answer: It allows query optimization without exposing partition columns to users, unlike Hive-style partitioning.
  id: 194

- stack: spark
  type: theory
  topic: Apache Iceberg
  difficulty: hard
  question: How does Iceberg enable snapshot isolation for queries?
  answer: By keeping metadata about snapshots and ensuring queries operate on consistent versions of data.
  id: 195

- stack: spark
  type: theory
  topic: Apache Hudi
  difficulty: easy
  question: What is the difference between Hudi Copy-on-Write and Merge-on-Read tables?
  answer: Copy-on-Write rewrites entire files on updates, while Merge-on-Read defers updates until read time for better write performance.
  id: 196

- stack: spark
  type: theory
  topic: Apache Hudi
  difficulty: medium
  question: How does Apache Hudi support Change Data Capture (CDC)?
  answer: It tracks incremental commits and exposes data streams for downstream CDC pipelines.
  id: 197

- stack: spark
  type: theory
  topic: Lake House
  difficulty: hard
  question: Compare Delta Lake, Iceberg, and Hudi in terms of update and delete support.
  answer: Delta Lake and Hudi support upserts and deletes directly, while Iceberg focuses on snapshot-based versioning and metadata efficiency.
  id: 198

- stack: spark
  type: theory
  topic: Lake House
  difficulty: medium
  question: Which Lakehouse format is better for streaming workloads?
  answer: Delta Lake and Hudi are often better suited for streaming workloads due to native upsert and incremental processing support.
  id: 199

- stack: spark
  type: theory
  topic: Incremental ETL
  difficulty: easy
  question: What is Incremental ETL in Spark?
  answer: It refers to processing only new or changed data instead of reprocessing the entire dataset.
  id: 200

- stack: spark
  type: theory
  topic: Incremental ETL
  difficulty: medium
  question: How can watermarks help in incremental ETL pipelines?
  answer: Watermarks define event-time thresholds to handle late data and ensure state cleanup in streaming ETL.
  id: 201

- stack: spark
  type: theory
  topic: Incremental ETL
  difficulty: medium
  question: How do checkpointing and write-ahead logs support incremental ETL?
  answer: They allow Spark to resume processing from the last successful state after failures.
  id: 202

- stack: spark
  type: theory
  topic: Data Capture
  difficulty: easy
  question: What is Change Data Capture (CDC) in Spark?
  answer: CDC refers to identifying and processing inserts, updates, and deletes from source systems.
  id: 203

- stack: spark
  type: theory
  topic: Data Capture
  difficulty: medium
  question: How can Spark Structured Streaming be used for CDC pipelines?
  answer: It can continuously ingest changes from sources like Kafka, apply transformations, and write results to data lakes or warehouses.
  id: 204

- stack: spark
  type: theory
  topic: Data Capture
  difficulty: hard
  question: What are the challenges of implementing CDC with Spark?
  answer: Challenges include handling schema evolution, ensuring exactly-once delivery, managing late events, and scaling with high change rates.
  id: 205

- stack: spark
  type: theory
  topic: Incremental ETL
  difficulty: hard
  question: How do merge operations in Delta Lake support incremental ETL?
  answer: Delta Lakeâ€™s MERGE INTO allows upserts of changed records, making it efficient for incremental pipelines.
  id: 206

- stack: spark
  type: theory
  topic: Incremental ETL
  difficulty: medium
  question: Why is partitioning strategy important in incremental ETL?
  answer: A good partitioning strategy reduces shuffle, speeds up queries, and avoids rewriting large volumes of data unnecessarily.
  id: 207

- stack: spark
  type: theory
  topic: Incremental ETL
  difficulty: hard
  question: How can Spark handle CDC when the source system provides only snapshots and not logs?
  answer: Spark can compare snapshots using join/diff strategies or leverage Hudi/Iceberg metadata for incremental detection.
  id: 208
